{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from glob import glob\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import wqet_grader\n",
    "from category_encoders import OneHotEncoder\n",
    "from IPython.display import VimeoVideo\n",
    "from sklearn.linear_model import LinearRegression, Ridge  # noqa F401\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.utils.validation import check_is_fitted\n",
    "\n",
    "warnings.simplefilter(action=\"ignore\", category=FutureWarning)\n",
    "wqet_grader.init(\"Project 2 Assessment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the last lesson, we used our wrangle function to import two CSV files as DataFrames. But what if we had hundreds of CSV files to import? Wrangling them one-by-one wouldn't be an option. So let's start with a technique for reading several CSV files into a single DataFrame.\n",
    "\n",
    "The first step is to gather the names of all the files we want to import. We can do this using pattern matching.\n",
    "\n",
    "Task 2.3.1: Use glob to create a list that contains the filenames for all the Buenos Aires real estate CSV files in the data directory. Assign this list to the variable name files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = glob(\"data/buenos-aires-real-estate-*.csv\")\n",
    "files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 2.3.2: Use your wrangle function in a for loop to create a list named frames. The list should the cleaned DataFrames created from the CSV filenames your collected in files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = []\n",
    "for file in files:\n",
    "    df = wrangle(file)\n",
    "    frames.append(df)\n",
    "\n",
    "frames[3].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 2.3.3: Use pd.concat to concatenate the items in frames into a single DataFrame df. Make sure you set the ignore_index argument to True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat(frames, ignore_index=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore\n",
    "\n",
    "Looking through the output from the df.head() call above, there's a little bit more cleaning we need to do before we can work with the neighborhood information in this dataset. The good news is that, because we're using a wrangle function, we only need to change the function to re-clean all of our CSV files. This is why functions are so useful.\n",
    "\n",
    "Task 2.3.4: Modify your wrangle function to create a new feature \"neighborhood\". You can find the neighborhood for each property in the \"place_with_parent_names\" column. For example, a property with the place name \"|Argentina|Capital Federal|Palermo|\" is located in the neighborhood is \"Palermo\". Also, your function should drop the \"place_with_parent_names\" column.\n",
    "\n",
    "Be sure to rerun all the cells above before you continue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split\n",
    "At this point, you should feel more comfortable with the splitting data, so we're going to condense the whole process down to one task.\n",
    "Task 2.3.5: Create your feature matrix X_train and target vector y_train. X_train should contain one feature: \"neighborhood\". Your target is \"price_aprox_usd\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = \"price_aprox_usd\"\n",
    "features = [\"neighborhood\"]\n",
    "y_train = df[target]\n",
    "X_train = df[features]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Model\n",
    "#### Baseline\n",
    "\n",
    "Let's also condense the code we use to establish our baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_mean = y_train.mean()\n",
    "y_pred_baseline = [y_mean] * len(y_train)\n",
    "print(\"Mean apt price:\", y_mean) \n",
    "\n",
    "print(\"Baseline MAE:\", mean_absolute_error(y_train, y_pred_baseline))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterate\n",
    "\n",
    "If you try to fit a LinearRegression predictor to your training data at this point, you'll get an error that looks like this:\n",
    "\n",
    "ValueError: could not convert string to float\n",
    "What does this mean? When you fit a linear regression model, you're asking scikit-learn to perform a mathematical operation. The problem is that our training set contains neighborhood information in non-numerical form. In order to create our model we need to encode that information so that it's represented numerically. The good news is that there are lots of transformers that can do this. Here, we'll use the one from the Category Encoders library, called a OneHotEncoder.\n",
    "\n",
    "Before we build include this transformer in our pipeline, let's explore how it works.\n",
    "\n",
    "Task 2.3.7: First, instantiate a OneHotEncoder named ohe. Make sure to set the use_cat_names argument to True. Next, fit your transformer to the feature matrix X_train. Finally, use your encoder to transform the feature matrix X_train, and assign the transformed data to the variable XT_train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#instantiate\n",
    "ohe = OneHotEncoder(use_cat_names = True)\n",
    "\n",
    "#Fit\n",
    "ohe.fit(X_train)\n",
    "\n",
    "#Transform\n",
    "XT_train = ohe.transform(X_train)\n",
    "\n",
    "print(XT_train.shape)\n",
    "XT_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 2.3.8: Create a pipeline named model that contains a OneHotEncoder transformer and a LinearRegression predictor. Then fit your model to the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = make_pipeline(\n",
    "OneHotEncoder(use_cat_names=True),\n",
    "Ridge()\n",
    ") model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate\n",
    "Regardless of how you build your model, the evaluation step stays the same. Let's see how our model performs with the training set.\n",
    "\n",
    "Task 2.3.9: First, create a list of predictions for the observations in your feature matrix X_train. Name this list y_pred_training. Then calculate the training mean absolute error for your predictions in y_pred_training as compared to the true targets in y_train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_training = model.predict(X_train)\n",
    "mae_training = mean_absolute_error(y_train, y_pred_training)\n",
    "print(\"Training MAE:\", round(mae_training, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 2.3.10: Run the code below to import your test data buenos-aires-test-features.csv into a DataFrame and generate a Series of predictions using your model. Then run the following cell to submit your predictions to the grader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = pd.read_csv(\"data/buenos-aires-test-features.csv\")[features]\n",
    "y_pred_test = pd.Series(model.predict(X_test))\n",
    "y_pred_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Communicating Result\n",
    "\n",
    "If we write out the equation for our model, it'll be too big to fit on the screen. That's because, when we used the OneHotEncoder to encode the neighborhood data, we created a much wider DataFrame, and each column/feature has it's own coefficient in our model's equation.\n",
    "This is important to keep in mind for two reasons. First, it means that this is a high-dimensional model. Instead of a 2D or 3D plot, we'd need a 58-dimensional plot to represent it, which is impossible! Second, it means that we'll need to extract and represent the information for our equation a little differently than before. Let's start by getting our intercept and coefficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intercept = model.named_steps[\"ridge\"].intercept_\n",
    "coefficients = model.named_steps[\"ridge\"].coef_\n",
    "print(\"coefficients len:\", len(coefficients))\n",
    "print(coefficients[:5])  # First five coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check your work\n",
    "assert isinstance(\n",
    "    intercept, float\n",
    "), f\"`intercept` should be a `float`, not {type(intercept)}.\"\n",
    "assert isinstance(\n",
    "    coefficients, np.ndarray\n",
    "), f\"`coefficients` should be a `float`, not {type(coefficients)}.\"\n",
    "assert coefficients.shape == (\n",
    "    57,\n",
    "), f\"`coefficients` is wrong shape: {coefficients.shape}.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = model.named_steps[\"onehotencoder\"].get_feature_names()\n",
    "print(\"features len:\", len(feature_names))\n",
    "print(feature_names[:5])  # First five feature names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check your work\n",
    "assert isinstance(\n",
    "    feature_names, np.ndarray\n",
    "), f\"`features` should be a `list`, not {type(feature_names)}.\"\n",
    "assert len(feature_names) == len(\n",
    "    coefficients\n",
    "), \"You should have the same number of features and coefficients.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2.3.13:** Create a pandas Series named `feat_imp` where the index is your `features` and the values are your `coefficients`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_imp = pd.Series(coefficients, index=feature_names )\n",
    "feat_imp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check your work\n",
    "assert isinstance(\n",
    "    feat_imp, pd.Series\n",
    "), f\"`feat_imp` should be a `float`, not {type(feat_imp)}.\"\n",
    "assert feat_imp.shape == (57,), f\"`feat_imp` is wrong shape: {feat_imp.shape}.\"\n",
    "assert all(\n",
    "    a == b for a, b in zip(sorted(feature_names), sorted(feat_imp.index))\n",
    "), \"The index of `feat_imp` should be identical to `features`.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be clear, it's definitely not a good idea to show this long equation to an audience, but let's print it out just to check our work. Since there are so many terms to print, we'll use a for loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"price = {intercept.round(2)}\")\n",
    "for f, c in feat_imp.items():\n",
    "    print(f\"+ ({round(c, 2)} * {f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Warning: In the first lesson for this project, we said that you shouldn't make any changes to your model after you see your test metrics. That's still true. However, we're breaking that rule here so that we can discuss overfitting. In future lessons, you'll learn how to protect against overfitting without checking your test set.\n",
    "\n",
    "Task 2.3.15: Scroll up, change the predictor in your model to Ridge, and retrain it. Then evaluate the model's training and test performance. Do you still have an overfitting problem? If not, extract the intercept and coefficients again (you'll need to change your code a little bit) and regenerate the model's equation. Does it look different than before?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 2.3.16: Create a horizontal bar chart that shows the top 15 coefficients for your model, based on their absolute value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sort by absolute values\n",
    "feat_imp.sort_values(key=abs).tail(15).plot(kind=\"barh\")\n",
    "plt.xlabel(\"Importance[USD]\")\n",
    "plt.ylabel(\"Feature\")\n",
    "plt.title(\"Feature Importance for Apartment Price\");"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
