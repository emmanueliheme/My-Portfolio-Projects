{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "4.4. Beyond the Model: Data Ethics\n",
    "\n",
    "import sqlite3\n",
    "import warnings\n",
    "​\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from category_encoders import OneHotEncoder\n",
    "from IPython.display import VimeoVideo\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.utils.validation import check_is_fitted\n",
    "​\n",
    "warnings.simplefilter(action=\"ignore\", category=FutureWarning)\n",
    "VimeoVideo(\"665414155\", h=\"c8a3e81a05\", width=600)\n",
    "Prepare Data\n",
    "Task 4.4.1: Run the cell below to connect to the nepal.sqlite database.\n",
    "\n",
    "What's ipython-sql?\n",
    "What's a Magics function?\n",
    "%load_ext sql\n",
    "%sql sqlite:////home/jovyan/nepal.sqlite\n",
    "There's a new jupysql version available (0.10.16), you're running 0.10.1. To upgrade: pip install jupysql --upgrade\n",
    "VimeoVideo(\"665415362\", h=\"f677c48c46\", width=600)\n",
    "Task 4.4.2: Select all columns from the household_demographics table, limiting your results to the first five rows.\n",
    "\n",
    "Write a basic query in SQL.\n",
    "Inspect a table using a LIMIT clause in SQL.\n",
    "%%sql\n",
    "Select *\n",
    "From household_demographics\n",
    "Limit 5\n",
    "​\n",
    "Running query in 'sqlite:////home/jovyan/nepal.sqlite'\n",
    "household_id\tgender_household_head\tage_household_head\tcaste_household\teducation_level_household_head\tincome_level_household\tsize_household\tis_bank_account_present_in_household\n",
    "101\tMale\t31.0\tRai\tIlliterate\tRs. 10 thousand\t3.0\t0.0\n",
    "201\tFemale\t62.0\tRai\tIlliterate\tRs. 10 thousand\t6.0\t0.0\n",
    "301\tMale\t51.0\tGharti/Bhujel\tIlliterate\tRs. 10 thousand\t13.0\t0.0\n",
    "401\tMale\t48.0\tGharti/Bhujel\tIlliterate\tRs. 10 thousand\t5.0\t0.0\n",
    "501\tMale\t70.0\tGharti/Bhujel\tIlliterate\tRs. 10 thousand\t8.0\t0.0\n",
    "Task 4.4.3: How many observations are in the household_demographics table? Use the count command to find out.\n",
    "\n",
    "Calculate the number of rows in a table using a count function in SQL. WQU WorldQuant University Applied Data Science Lab QQQQ\n",
    "%%sql\n",
    "Select count (*)\n",
    "​\n",
    "From household_demographics\n",
    "​\n",
    "Running query in 'sqlite:////home/jovyan/nepal.sqlite'\n",
    "count (*)\n",
    "249932\n",
    "VimeoVideo(\"665415378\", h=\"aa2b99493e\", width=600)\n",
    "Task 4.4.4: Select all columns from the id_map table, limiting your results to the first five rows.\n",
    "\n",
    "Inspect a table using a LIMIT clause in SQL.\n",
    "What columns does it have in common with household_demographics that we can use to join them?\n",
    "\n",
    "%%sql\n",
    "SELECT *\n",
    "FROM id_map\n",
    "LIMIT 5\n",
    "​\n",
    "Running query in 'sqlite:////home/jovyan/nepal.sqlite'\n",
    "household_id\tbuilding_id\tvdcmun_id\tdistrict_id\n",
    "5601\t56\t7\t1\n",
    "6301\t63\t7\t1\n",
    "9701\t97\t7\t1\n",
    "9901\t99\t7\t1\n",
    "11501\t115\t7\t1\n",
    "VimeoVideo(\"665415406\", h=\"46a990c8f7\", width=600)\n",
    "Task 4.4.5: Create a table with all the columns from household_demographics, all the columns from building_structure, the vdcmun_id column from id_map, and the damage_grade column from building_damage. Your results should show only rows where the district_id is 4 and limit your results to the first five rows.\n",
    "\n",
    "Create an alias for a column or table using the AS command in SQL.\n",
    "Determine the unique values in a column using a DISTINCT function in SQL.\n",
    "Merge two tables using a JOIN clause in SQL.\n",
    "Inspect a table using a LIMIT clause in SQL.\n",
    "Subset a table using a WHERE clause in SQL.\n",
    "%%sql\n",
    "SELECT  h.*, \n",
    "        s.*,\n",
    "        i.vdcmun_id, \n",
    "        d.damage_grade\n",
    "FROM household_demographics AS h\n",
    "JOIN id_map AS i ON i.household_id = h.household_id\n",
    "JOIN building_structure AS s ON i.building_id = s.building_id\n",
    "JOIN building_damage AS d ON i.building_id = d.building_id\n",
    "WHERE district_id = 4\n",
    "​\n",
    "LIMIT 5\n",
    "​\n",
    "​\n",
    "Running query in 'sqlite:////home/jovyan/nepal.sqlite'\n",
    "household_id\tgender_household_head\tage_household_head\tcaste_household\teducation_level_household_head\tincome_level_household\tsize_household\tis_bank_account_present_in_household\tbuilding_id\tcount_floors_pre_eq\tcount_floors_post_eq\tage_building\tplinth_area_sq_ft\theight_ft_pre_eq\theight_ft_post_eq\tland_surface_condition\tfoundation_type\troof_type\tground_floor_type\tother_floor_type\tposition\tplan_configuration\tcondition_post_eq\tsuperstructure\tvdcmun_id\tdamage_grade\n",
    "16400201\tFemale\t46.0\tChhetree\tClass 5\tRs. 10-20 thousand\t4.0\t1.0\t164002\t3\t3\t20\t560\t18\t18\tFlat\tMud mortar-Stone/Brick\tBamboo/Timber-Light roof\tMud\tTImber/Bamboo-Mud\tNot attached\tRectangular\tDamaged-Repaired and used\tStone, mud mortar\t38\tGrade 2\n",
    "16408101\tMale\t66.0\tChhetree\tIlliterate\tRs. 10 thousand\t5.0\t0.0\t164081\t2\t2\t21\t200\t12\t12\tFlat\tMud mortar-Stone/Brick\tBamboo/Timber-Light roof\tMud\tTImber/Bamboo-Mud\tNot attached\tRectangular\tDamaged-Used in risk\tStone, mud mortar\t38\tGrade 2\n",
    "16408901\tMale\t54.0\tMagar\tClass 4\tRs. 10 thousand\t5.0\t1.0\t164089\t3\t3\t18\t315\t20\t20\tFlat\tMud mortar-Stone/Brick\tBamboo/Timber-Light roof\tMud\tTImber/Bamboo-Mud\tNot attached\tRectangular\tDamaged-Used in risk\tStone, mud mortar\t38\tGrade 2\n",
    "16409801\tMale\t36.0\tChhetree\tClass 5\tRs. 10 thousand\t6.0\t1.0\t164098\t2\t2\t45\t290\t13\t13\tFlat\tMud mortar-Stone/Brick\tBamboo/Timber-Light roof\tMud\tTImber/Bamboo-Mud\tNot attached\tRectangular\tDamaged-Used in risk\tStone, mud mortar\t38\tGrade 3\n",
    "16410301\tFemale\t39.0\tChhetree\tClass 4\tRs. 10 thousand\t3.0\t0.0\t164103\t2\t2\t21\t230\t13\t13\tFlat\tMud mortar-Stone/Brick\tBamboo/Timber-Light roof\tMud\tTImber/Bamboo-Mud\tNot attached\tRectangular\tDamaged-Used in risk\tStone, mud mortar\t38\tGrade 3\n",
    "Import\n",
    "def wrangle(db_path):\n",
    "    # Connect to database\n",
    "    conn = sqlite3.connect(db_path)\n",
    "​\n",
    "    # Construct query\n",
    "    query = \"\"\"\n",
    "    SELECT  h.*, \n",
    "            s.*,\n",
    "            i.vdcmun_id, \n",
    "            d.damage_grade\n",
    "    FROM household_demographics AS h\n",
    "    JOIN id_map AS i ON i.household_id = h.household_id\n",
    "    JOIN building_structure AS s ON i.building_id = s.building_id\n",
    "    JOIN building_damage AS d ON i.building_id = d.building_id\n",
    "    WHERE district_id = 4\n",
    "        \"\"\"\n",
    "​\n",
    "    # Read query results into DataFrame\n",
    "    df = pd.read_sql(query, conn, index_col=\"household_id\")\n",
    "​\n",
    "    # Identify leaky columns\n",
    "    drop_cols = [col for col in df.columns if \"post_eq\" in col]\n",
    "​\n",
    "    # Add high-cardinality / redundant column\n",
    "    drop_cols.append(\"building_id\")\n",
    "​\n",
    "    # Create binary target column\n",
    "    df[\"damage_grade\"] = df[\"damage_grade\"].str[-1].astype(int)\n",
    "    df[\"severe_damage\"] = (df[\"damage_grade\"] > 3).astype(int)\n",
    "​\n",
    "    # Drop old target\n",
    "    drop_cols.append(\"damage_grade\")\n",
    "​\n",
    "    # Drop multicollinearity column\n",
    "    drop_cols.append(\"count_floors_pre_eq\")\n",
    "    \n",
    "    # Drop columns\n",
    "    df.drop(columns=drop_cols, inplace=True)\n",
    "    \n",
    "    #Group Caste_household\n",
    "    top_10 = df[\"caste_household\"].value_counts().head(10).index\n",
    "    df[\"caste_household\"] = df[\"caste_household\"].apply(\n",
    "        lambda c: c if c in top_10 else \"other\"\n",
    "    )\n",
    "​\n",
    "    return df\n",
    "VimeoVideo(\"665415443\", h=\"ca27a7ebfc\", width=600)\n",
    "Task 4.4.6: Add the query you created in the previous task to the wrangle function above. Then import your data by running the cell below. The path to the database is \"/home/jovyan/nepal.sqlite\".\n",
    "\n",
    "Read SQL query into a DataFrame using pandas.\n",
    "Write a function in Python.\n",
    "df = wrangle(\"/home/jovyan/nepal.sqlite\")\n",
    "df.head()\n",
    "gender_household_head\tage_household_head\tcaste_household\teducation_level_household_head\tincome_level_household\tsize_household\tis_bank_account_present_in_household\tage_building\tplinth_area_sq_ft\theight_ft_pre_eq\tland_surface_condition\tfoundation_type\troof_type\tground_floor_type\tother_floor_type\tposition\tplan_configuration\tsuperstructure\tvdcmun_id\tsevere_damage\n",
    "household_id\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n",
    "16400201\tFemale\t46.0\tChhetree\tClass 5\tRs. 10-20 thousand\t4.0\t1.0\t20\t560\t18\tFlat\tMud mortar-Stone/Brick\tBamboo/Timber-Light roof\tMud\tTImber/Bamboo-Mud\tNot attached\tRectangular\tStone, mud mortar\t38\t0\n",
    "16408101\tMale\t66.0\tChhetree\tIlliterate\tRs. 10 thousand\t5.0\t0.0\t21\t200\t12\tFlat\tMud mortar-Stone/Brick\tBamboo/Timber-Light roof\tMud\tTImber/Bamboo-Mud\tNot attached\tRectangular\tStone, mud mortar\t38\t0\n",
    "16408901\tMale\t54.0\tMagar\tClass 4\tRs. 10 thousand\t5.0\t1.0\t18\t315\t20\tFlat\tMud mortar-Stone/Brick\tBamboo/Timber-Light roof\tMud\tTImber/Bamboo-Mud\tNot attached\tRectangular\tStone, mud mortar\t38\t0\n",
    "16409801\tMale\t36.0\tChhetree\tClass 5\tRs. 10 thousand\t6.0\t1.0\t45\t290\t13\tFlat\tMud mortar-Stone/Brick\tBamboo/Timber-Light roof\tMud\tTImber/Bamboo-Mud\tNot attached\tRectangular\tStone, mud mortar\t38\t0\n",
    "16410301\tFemale\t39.0\tChhetree\tClass 4\tRs. 10 thousand\t3.0\t0.0\t21\t230\t13\tFlat\tMud mortar-Stone/Brick\tBamboo/Timber-Light roof\tMud\tTImber/Bamboo-Mud\tNot attached\tRectangular\tStone, mud mortar\t38\t0\n",
    "# Check your work\n",
    "assert df.shape == (75883, 20), f\"`df` should have shape (75883, 20), not {df.shape}\"\n",
    "Explore\n",
    "VimeoVideo(\"665415463\", h=\"86c306199f\", width=600)\n",
    "Task 4.4.7: Combine the select_dtypes and nunique methods to see if there are any high- or low-cardinality categorical features in the dataset.\n",
    "\n",
    "What are high- and low-cardinality features?\n",
    "Determine the unique values in a column using pandas.\n",
    "Subset a DataFrame's columns based on the column data types in pandas.\n",
    "# Check for high- and low-cardinality categorical features\n",
    "df.select_dtypes(\"object\").nunique()\n",
    "gender_household_head              2\n",
    "caste_household                   63\n",
    "education_level_household_head    19\n",
    "income_level_household             5\n",
    "land_surface_condition             3\n",
    "foundation_type                    5\n",
    "roof_type                          3\n",
    "ground_floor_type                  5\n",
    "other_floor_type                   4\n",
    "position                           4\n",
    "plan_configuration                10\n",
    "superstructure                    11\n",
    "dtype: int64\n",
    "df.select_dtypes(\"object\").nunique()\n",
    "gender_household_head              2\n",
    "caste_household                   11\n",
    "education_level_household_head    19\n",
    "income_level_household             5\n",
    "land_surface_condition             3\n",
    "foundation_type                    5\n",
    "roof_type                          3\n",
    "ground_floor_type                  5\n",
    "other_floor_type                   4\n",
    "position                           4\n",
    "plan_configuration                10\n",
    "superstructure                    11\n",
    "dtype: int64\n",
    "VimeoVideo(\"665415472\", h=\"1142d69e4a\", width=600)\n",
    "when you have a group or groups that has only representation in your dataset what data scentist do is to aggregate them into a category called \"others\". this is because it resulted to a high cardinality\n",
    "\n",
    "Task 4.4.8: Add to your wrangle function so that the \"caste_household\" contains only the 10 largest caste groups. For the rows that are not in those groups, \"caste_household\" should be changed to \"Other\".\n",
    "\n",
    "Determine the unique values in a column using pandas.\n",
    "Combine multiple categories in a Series using pandas.\n",
    "#top_10 = df[\"caste_household\"].value_counts().head(10).index\n",
    "#df[\"caste_household\"].apply(lambda c: c if c in top_10 else \"other\").value_counts()\n",
    "​\n",
    "Index(['Gurung', 'Brahman-Hill', 'Chhetree', 'Magar', 'Sarki', 'Newar', 'Kami',\n",
    "       'Tamang', 'Kumal', 'Damai/Dholi'],\n",
    "      dtype='object')\n",
    "#df[\"caste_household\"].apply(lambda c: c if c in top_10 else \"other\").value_counts()\n",
    "Gurung          15119\n",
    "Brahman-Hill    13043\n",
    "Chhetree         8766\n",
    "other            8608\n",
    "Magar            8180\n",
    "Sarki            6052\n",
    "Newar            5906\n",
    "Kami             3565\n",
    "Tamang           2396\n",
    "Kumal            2271\n",
    "Damai/Dholi      1977\n",
    "Name: caste_household, dtype: int64\n",
    "# Check your work\n",
    "assert (\n",
    "    df[\"caste_household\"].nunique() == 11\n",
    "), f\"The `'caste_household'` column should only have 11 unique values, not {df['caste_household'].nunique()}.\"\n",
    "Split\n",
    "VimeoVideo(\"665415515\", h=\"defc252edd\", width=600)\n",
    "Task 4.4.9: Create your feature matrix X and target vector y. Since our model will only consider building and household data, X should not include the municipality column \"vdcmun_id\". Your target is \"severe_damage\".\n",
    "\n",
    "target = \"severe_damage\"\n",
    "X = df.drop(columns = [target, \"vdcmun_id\"])\n",
    "y = df[target]\n",
    "# Check your work\n",
    "assert X.shape == (75883, 18), f\"The shape of `X` should be (75883, 18), not {X.shape}.\"\n",
    "assert \"vdcmun_id\" not in X.columns, \"There should be no `'vdcmun_id'` column in `X`.\"\n",
    "assert y.shape == (75883,), f\"The shape of `y` should be (75883,), not {y.shape}.\"\n",
    "Task 4.4.10: Divide your data (X and y) into training and test sets using a randomized train-test split. Your test set should be 20% of your total data. Be sure to set a random_state for reproducibility.\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size = 0.2, random_state = 42\n",
    ")\n",
    "# Check your work\n",
    "assert X_train.shape == (\n",
    "    60706,\n",
    "    18,\n",
    "), f\"The shape of `X_train` should be (60706, 18), not {X_train.shape}.\"\n",
    "assert y_train.shape == (\n",
    "    60706,\n",
    "), f\"The shape of `y_train` should be (60706,), not {y_train.shape}.\"\n",
    "assert X_test.shape == (\n",
    "    15177,\n",
    "    18,\n",
    "), f\"The shape of `X_test` should be (15177, 18), not {X_test.shape}.\"\n",
    "assert y_test.shape == (\n",
    "    15177,\n",
    "), f\"The shape of `y_test` should be (15177,), not {y_test.shape}.\"\n",
    "Build Model\n",
    "Baseline\n",
    "Task 4.4.11: Calculate the baseline accuracy score for your model.\n",
    "\n",
    "What's accuracy score?\n",
    "Aggregate data in a Series using value_counts in pandas.\n",
    "acc_baseline = y_train.value_counts(normalize=True).max()\n",
    "print(\"Baseline Accuracy:\", round(acc_baseline, 2))\n",
    "Baseline Accuracy: 0.63\n",
    "Iterate\n",
    "Task 4.4.12: Create a Pipeline called model_lr. It should have an OneHotEncoder transformer and a LogisticRegression predictor. Be sure you set the use_cat_names argument for your transformer to True.\n",
    "\n",
    "What's logistic regression?\n",
    "What's one-hot encoding?\n",
    "Create a pipeline in scikit-learn.\n",
    "Fit a model to training data in scikit-learn.\n",
    "model_lr = make_pipeline(\n",
    "    OneHotEncoder(use_cat_names=True),\n",
    "    LogisticRegression(max_iter=3000)\n",
    ")\n",
    "# Fit model to training data\n",
    "model_lr.fit(X_train,y_train)\n",
    "​\n",
    "Pipeline(steps=[('onehotencoder',\n",
    "                 OneHotEncoder(cols=['gender_household_head', 'caste_household',\n",
    "                                     'education_level_household_head',\n",
    "                                     'income_level_household',\n",
    "                                     'land_surface_condition',\n",
    "                                     'foundation_type', 'roof_type',\n",
    "                                     'ground_floor_type', 'other_floor_type',\n",
    "                                     'position', 'plan_configuration',\n",
    "                                     'superstructure'],\n",
    "                               use_cat_names=True)),\n",
    "                ('logisticregression', LogisticRegression(max_iter=3000))])\n",
    "In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook.\n",
    "On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\n",
    "# Check your work\n",
    "assert isinstance(\n",
    "    model_lr, Pipeline\n",
    "), f\"`model_lr` should be a Pipeline, not type {type(model_lr)}.\"\n",
    "assert isinstance(\n",
    "    model_lr[0], OneHotEncoder\n",
    "), f\"The first step in your Pipeline should be a OneHotEncoder, not type {type(model_lr[0])}.\"\n",
    "assert isinstance(\n",
    "    model_lr[-1], LogisticRegression\n",
    "), f\"The last step in your Pipeline should be LogisticRegression, not type {type(model_lr[-1])}.\"\n",
    "check_is_fitted(model_lr)\n",
    "Evaluate\n",
    "Task 4.4.13: Calculate the training and test accuracy scores for model_lr.\n",
    "\n",
    "Calculate the accuracy score for a model in scikit-learn.\n",
    "Generate predictions using a trained model in scikit-learn.\n",
    "#calc acc score\n",
    "model_lr.score(X_test, y_test)\n",
    "0.7229360216116492\n",
    "acc_train = accuracy_score(y_train, model_lr.predict(X_train))\n",
    "acc_test = accuracy_score(y_test, model_lr.predict(X_test))\n",
    "​\n",
    "print(\"LR Training Accuracy:\", acc_train)\n",
    "print(\"LR Validation Accuracy:\", acc_test)\n",
    "LR Training Accuracy: 0.7178367871380095\n",
    "LR Validation Accuracy: 0.7229360216116492\n",
    "Communicate\n",
    "VimeoVideo(\"665415532\", h=\"00440f76a9\", width=600)\n",
    "Task 4.4.14: First, extract the feature names and importances from your model. Then create a pandas Series named feat_imp, where the index is features and the values are your the exponential of the importances.\n",
    "\n",
    "What's a bar chart?\n",
    "Access an object in a pipeline in scikit-learn.\n",
    "Create a Series in pandas.\n",
    "features = model_lr.named_steps[\"onehotencoder\"].get_feature_names()\n",
    "importances = model_lr.named_steps[\"logisticregression\"].coef_[0]\n",
    "feat_imp = pd.Series(np.exp(importances), index=features).sort_values()\n",
    "feat_imp.head()\n",
    "superstructure_Brick, cement mortar    0.351394\n",
    "foundation_type_RC                     0.353164\n",
    "roof_type_RCC/RB/RBC                   0.389604\n",
    "other_floor_type_RCC/RB/RBC            0.522843\n",
    "caste_household_Kumal                  0.528630\n",
    "dtype: float64\n",
    "VimeoVideo(\"665415552\", h=\"5b2383ccf8\", width=600)\n",
    "Task 4.4.15: Create a horizontal bar chart with the ten largest coefficients from feat_imp. Be sure to label your x-axis \"Odds Ratio\".\n",
    "\n",
    "Create a bar chart using pandas.\n",
    "feat_imp.tail(10).plot(kind=\"barh\")\n",
    "plt.xlabel(\"odds Ratio\");\n",
    "\n",
    "VimeoVideo(\"665415581\", h=\"d15477e14d\", width=600)\n",
    "Task 4.4.16: Create a horizontal bar chart with the ten smallest coefficients from feat_imp. Be sure to label your x-axis \"Odds Ratio\".\n",
    "\n",
    "Create a bar chart using pandas.\n",
    "feat_imp.head(10).plot(kind=\"barh\")\n",
    "plt.xlabel(\"odds Ratio\");\n",
    "\n",
    "Explore Some More\n",
    "VimeoVideo(\"665415631\", h=\"90ba264392\", width=600)\n",
    "Task 4.4.17: Which municipalities saw the highest proportion of severely damaged buildings? Create a DataFrame damage_by_vdcmun by grouping df by \"vdcmun_id\" and then calculating the mean of the \"severe_damage\" column. Be sure to sort damage_by_vdcmun from highest to lowest proportion.\n",
    "\n",
    "Aggregate data using the groupby method in pandas.\n",
    "damage_by_vdcmun = (\n",
    "    df.groupby(\"vdcmun_id\")[\"severe_damage\"].mean().sort_values(ascending=False)\n",
    ").to_frame()\n",
    "damage_by_vdcmun\n",
    "severe_damage\n",
    "vdcmun_id\t\n",
    "31\t0.930199\n",
    "32\t0.851117\n",
    "35\t0.827145\n",
    "30\t0.824201\n",
    "33\t0.782464\n",
    "34\t0.666979\n",
    "39\t0.572344\n",
    "40\t0.512444\n",
    "38\t0.506425\n",
    "36\t0.503972\n",
    "37\t0.437789\n",
    "# Check your work\n",
    "assert isinstance(\n",
    "    damage_by_vdcmun, pd.DataFrame\n",
    "), f\"`damage_by_vdcmun` should be a Series, not type {type(damage_by_vdcmun)}.\"\n",
    "assert damage_by_vdcmun.shape == (\n",
    "    11,\n",
    "    1,\n",
    "), f\"`damage_by_vdcmun` should be shape (11,1), not {damage_by_vdcmun.shape}.\"\n",
    "VimeoVideo(\"665415651\", h=\"9b5244dec1\", width=600)\n",
    "Task 4.4.18: Create a line plot of damage_by_vdcmun. Label your x-axis \"Municipality ID\", your y-axis \"% of Total Households\", and give your plot the title \"Household Damage by Municipality\".\n",
    "\n",
    "Create a line plot in Matplotlib.\n",
    "# Plot line\n",
    "plt.plot(damage_by_vdcmun.values, color=\"grey\")\n",
    "plt.xticks(range(len(damage_by_vdcmun)), labels=damage_by_vdcmun.index)\n",
    "plt.yticks(np.arange(0.0, 1.1, 0.2))\n",
    "plt.xlabel(\"Municipality ID\")\n",
    "plt.ylabel(\"% of Total Households\")\n",
    "plt.title(\"Severe Damage by Municipality\");\n",
    "\n",
    "Given the plot above, our next question is: How are the Gurung and Kumal populations distributed across these municipalities?\n",
    "\n",
    "VimeoVideo(\"665415693\", h=\"fb2e54aa04\", width=600)\n",
    "Task 4.4.19: Create a new column in damage_by_vdcmun that contains the the proportion of Gurung households in each municipality.\n",
    "\n",
    "Aggregate data using the groupby method in pandas.\n",
    "Create a Series in pandas.\n",
    "damage_by_vdcmun[\"Gurung\"] = (\n",
    "    df[df[\"caste_household\"] == \"Gurung\"].groupby(\"vdcmun_id\")[\"severe_damage\"].count()\n",
    "    /df.groupby(\"vdcmun_id\")[\"severe_damage\"].count()\n",
    "​\n",
    ")\n",
    "damage_by_vdcmun\n",
    "severe_damage\tGurung\n",
    "vdcmun_id\t\t\n",
    "31\t0.930199\t0.326937\n",
    "32\t0.851117\t0.387849\n",
    "35\t0.827145\t0.826889\n",
    "30\t0.824201\t0.338152\n",
    "33\t0.782464\t0.011943\n",
    "34\t0.666979\t0.385084\n",
    "39\t0.572344\t0.097971\n",
    "40\t0.512444\t0.246727\n",
    "38\t0.506425\t0.049023\n",
    "36\t0.503972\t0.143178\n",
    "37\t0.437789\t0.050485\n",
    "VimeoVideo(\"665415707\", h=\"9b29c23434\", width=600)\n",
    "Task 4.4.20: Create a new column in damage_by_vdcmun that contains the the proportion of Kumal households in each municipality. Replace any NaN values in the column with 0.\n",
    "\n",
    "Aggregate data using the groupby method in pandas.\n",
    "Create a Series in pandas.\n",
    "damage_by_vdcmun[\"Kumal\"] = (\n",
    "    df[df[\"caste_household\"] == \"Kumal\"].groupby(\"vdcmun_id\")[\"severe_damage\"].count()\n",
    "    /df.groupby(\"vdcmun_id\")[\"severe_damage\"].count()\n",
    "​\n",
    ").fillna(0)\n",
    "damage_by_vdcmun\n",
    "​\n",
    "severe_damage\tGurung\tKumal\n",
    "vdcmun_id\t\t\t\n",
    "31\t0.930199\t0.326937\t0.000000\n",
    "32\t0.851117\t0.387849\t0.000000\n",
    "35\t0.827145\t0.826889\t0.000000\n",
    "30\t0.824201\t0.338152\t0.000000\n",
    "33\t0.782464\t0.011943\t0.029478\n",
    "34\t0.666979\t0.385084\t0.000000\n",
    "39\t0.572344\t0.097971\t0.000267\n",
    "40\t0.512444\t0.246727\t0.036973\n",
    "38\t0.506425\t0.049023\t0.100686\n",
    "36\t0.503972\t0.143178\t0.003282\n",
    "37\t0.437789\t0.050485\t0.048842\n",
    "VimeoVideo(\"665415729\", h=\"8d0712c306\", width=600)\n",
    "Task 4.4.21: Create a visualization that combines the line plot of severely damaged households you made above with a stacked bar chart showing the proportion of Gurung and Kumal households in each district. Label your x-axis \"Municipality ID\", your y-axis \"% of Total Households\".\n",
    "\n",
    "Create a bar chart using pandas.\n",
    "Drop a column from a DataFrame using pandas.\n",
    "\n",
    "\n",
    "damage_by_vdcmun.drop(columns=\"severe_damage\").plot(\n",
    "    kind=\"bar\", stacked=True)\n",
    "\n",
    "plt.plot(damage_by_vdcmun[\"severe_damage\"].values, color=\"grey\")\n",
    "plt.xticks(range(len(damage_by_vdcmun)), labels=damage_by_vdcmun.index)\n",
    "plt.yticks(np.arange(0.0, 1.1, 0.2))\n",
    "plt.xlabel(\"Municipality ID\")\n",
    "plt.ylabel(\"% of Total Households\")\n",
    "plt.title(\"Household Caste by Municipality\")\n",
    "plt.legend();"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
