{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "4.3. Predicting Damage with Decision Trees\n",
    "\n",
    "import sqlite3\n",
    "import warnings\n",
    "​\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from category_encoders import OrdinalEncoder\n",
    "from IPython.display import VimeoVideo\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.utils.validation import check_is_fitted\n",
    "​\n",
    "warnings.simplefilter(action=\"ignore\", category=FutureWarning)\n",
    "VimeoVideo(\"665414130\", h=\"71649d291e\", width=600)\n",
    "Prepare Data\n",
    "Import\n",
    "def wrangle(db_path):\n",
    "    # Connect to database\n",
    "    conn = sqlite3.connect(db_path)\n",
    "​\n",
    "    # Construct query\n",
    "    query =  \"\"\"\n",
    "        SELECT distinct(i.building_id) AS b_id,\n",
    "           s.*,\n",
    "           d.damage_grade\n",
    "        FROM id_map AS i\n",
    "        JOIN building_structure AS s ON i.building_id = s.building_id\n",
    "        JOIN building_damage AS d ON i.building_id = d.building_id\n",
    "        WHERE district_id = 4\n",
    "    \"\"\"\n",
    "​\n",
    "    # Read query results into DataFrame\n",
    "    df = pd.read_sql(query, conn, index_col=\"b_id\")\n",
    "​\n",
    "    # Identify leaky columns\n",
    "    drop_cols = [col for col in df.columns if \"post_eq\" in col]\n",
    "​\n",
    "    # Add high-cardinality / redundant column\n",
    "    drop_cols.append(\"building_id\")\n",
    "​\n",
    "    # Create binary target column\n",
    "    df[\"damage_grade\"] = df[\"damage_grade\"].str[-1].astype(int)\n",
    "    df[\"severe_damage\"] = (df[\"damage_grade\"] > 3).astype(int)\n",
    "​\n",
    "    # Drop old target\n",
    "    drop_cols.append(\"damage_grade\")\n",
    "​\n",
    "    # Drop multicollinearity column\n",
    "    drop_cols.append(\"count_floors_pre_eq\")\n",
    "​\n",
    "    # Drop columns\n",
    "    df.drop(columns=drop_cols, inplace=True)\n",
    "​\n",
    "    return df\n",
    "Task 4.3.1: Use the wrangle function above to import your data set into the DataFrame df. The path to the SQLite database is \"/home/jovyan/nepal.sqlite\"\n",
    "\n",
    "Read SQL query into a DataFrame using pandas.\n",
    "Write a function in Python.\n",
    "df = wrangle(\"/home/jovyan/nepal.sqlite\")\n",
    "df.head()\n",
    "age_building\tplinth_area_sq_ft\theight_ft_pre_eq\tland_surface_condition\tfoundation_type\troof_type\tground_floor_type\tother_floor_type\tposition\tplan_configuration\tsuperstructure\tsevere_damage\n",
    "b_id\t\t\t\t\t\t\t\t\t\t\t\t\n",
    "164002\t20\t560\t18\tFlat\tMud mortar-Stone/Brick\tBamboo/Timber-Light roof\tMud\tTImber/Bamboo-Mud\tNot attached\tRectangular\tStone, mud mortar\t0\n",
    "164081\t21\t200\t12\tFlat\tMud mortar-Stone/Brick\tBamboo/Timber-Light roof\tMud\tTImber/Bamboo-Mud\tNot attached\tRectangular\tStone, mud mortar\t0\n",
    "164089\t18\t315\t20\tFlat\tMud mortar-Stone/Brick\tBamboo/Timber-Light roof\tMud\tTImber/Bamboo-Mud\tNot attached\tRectangular\tStone, mud mortar\t0\n",
    "164098\t45\t290\t13\tFlat\tMud mortar-Stone/Brick\tBamboo/Timber-Light roof\tMud\tTImber/Bamboo-Mud\tNot attached\tRectangular\tStone, mud mortar\t0\n",
    "164103\t21\t230\t13\tFlat\tMud mortar-Stone/Brick\tBamboo/Timber-Light roof\tMud\tTImber/Bamboo-Mud\tNot attached\tRectangular\tStone, mud mortar\t0\n",
    "# Check your work\n",
    "assert df.shape[0] == 70836, f\"`df` should have 70,836 rows, not {df.shape[0]}.\"\n",
    "assert df.shape[1] == 12, f\"`df` should have 12 columns, not {df.shape[1]}.\"\n",
    "Split\n",
    "Task 4.3.2: Create your feature matrix X and target vector y. Your target is \"severe_damage\".\n",
    "\n",
    "What's a feature matrix?\n",
    "What's a target vector?\n",
    "Subset a DataFrame by selecting one or more columns in pandas.\n",
    "Select a Series from a DataFrame in pandas.\n",
    "target = \"severe_damage\"\n",
    "X = df.drop(columns=target)\n",
    "y = df[target]\n",
    "# Check your work\n",
    "assert X.shape == (70836, 11), f\"The shape of `X` should be (70836, 11), not {X.shape}.\"\n",
    "assert y.shape == (70836,), f\"The shape of `y` should be (70836,), not {y.shape}.\"\n",
    "VimeoVideo(\"665415006\", h=\"ecb1b87861\", width=600)\n",
    "#Training data is for training your model\n",
    "#validation set is for tuning your model\n",
    "#test set is for testing your model\n",
    "Task 4.3.3: Divide your data (X and y) into training and test sets using a randomized train-test split. Your test set should be 20% of your total data. And don't forget to set a random_state for reproducibility.\n",
    "\n",
    "Perform a randomized train-test split using scikit-learn.WQU WorldQuant University Applied Data Science Lab QQQQ\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "# Check your work\n",
    "assert X_train.shape == (\n",
    "    56668,\n",
    "    11,\n",
    "), f\"The shape of `X_train` should be (56668, 11), not {X_train.shape}.\"\n",
    "assert y_train.shape == (\n",
    "    56668,\n",
    "), f\"The shape of `y_train` should be (56668,), not {y_train.shape}.\"\n",
    "assert X_test.shape == (\n",
    "    14168,\n",
    "    11,\n",
    "), f\"The shape of `X_test` should be (14168, 11), not {X_test.shape}.\"\n",
    "assert y_test.shape == (\n",
    "    14168,\n",
    "), f\"The shape of `y_test` should be (14168,), not {y_test.shape}.\"\n",
    "Task 4.3.4: Divide your training data (X_train and y_train) into training and validation sets using a randomized train-test split. Your validation data should be 20% of the remaining data. Don't forget to set a random_state.\n",
    "\n",
    "What's a validation set?\n",
    "Perform a randomized train-test split using scikit-learn.\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train, y_train, test_size=0.2, random_state=42\n",
    ")\n",
    "# Check your work\n",
    "assert X_train.shape == (\n",
    "    45334,\n",
    "    11,\n",
    "), f\"The shape of `X_train` should be (45334, 11), not {X_train.shape}.\"\n",
    "assert y_train.shape == (\n",
    "    45334,\n",
    "), f\"The shape of `y_train` should be (45334,), not {y_train.shape}.\"\n",
    "assert X_val.shape == (\n",
    "    11334,\n",
    "    11,\n",
    "), f\"The shape of `X_val` should be (11334, 11), not {X_val.shape}.\"\n",
    "assert y_val.shape == (\n",
    "    11334,\n",
    "), f\"The shape of `y_val` should be (11334,), not {y_val.shape}.\"\n",
    "Build Model\n",
    "Baseline\n",
    "Task 4.3.5: Calculate the baseline accuracy score for your model.\n",
    "\n",
    "What's accuracy score?\n",
    "Aggregate data in a Series using value_counts in pandas.\n",
    "acc_baseline = y_train.value_counts(normalize=True).max()\n",
    "print(\"Baseline Accuracy:\", round(acc_baseline, 2))\n",
    "Baseline Accuracy: 0.64\n",
    "Iterate\n",
    "VimeoVideo(\"665415061\", h=\"6250826047\", width=600)\n",
    "VimeoVideo(\"665415109\", h=\"b3bb82651d\", width=600)\n",
    "#ordianl endcoding is appropriate for Decision Trees\n",
    "Task 4.3.6: Create a pipeline named model that contains a OrdinalEncoder transformer and a DecisionTreeClassifier predictor. (Be sure to set a random_state for your predictor.) Then fit your model to the training data.\n",
    "\n",
    "What's a decision tree?\n",
    "What's ordinal encoding?\n",
    "Create a pipeline in scikit-learn.\n",
    "Fit a model to training data in scikit-learn.\n",
    "# Build Model\n",
    "model = make_pipeline(\n",
    "    OrdinalEncoder(), \n",
    "    DecisionTreeClassifier(max_depth = 6, random_state = 42)\n",
    ")\n",
    "# Fit model to training data\n",
    "model.fit(X_train, y_train)\n",
    "Pipeline(steps=[('ordinalencoder',\n",
    "                 OrdinalEncoder(cols=['land_surface_condition',\n",
    "                                      'foundation_type', 'roof_type',\n",
    "                                      'ground_floor_type', 'other_floor_type',\n",
    "                                      'position', 'plan_configuration',\n",
    "                                      'superstructure'],\n",
    "                                mapping=[{'col': 'land_surface_condition',\n",
    "                                          'data_type': dtype('O'),\n",
    "                                          'mapping': Flat              1\n",
    "Moderate slope    2\n",
    "Steep slope       3\n",
    "NaN              -2\n",
    "dtype: int64},\n",
    "                                         {'col': 'foundation_type',\n",
    "                                          'dat...\n",
    "Others                              9\n",
    "Building with Central Courtyard    10\n",
    "NaN                                -2\n",
    "dtype: int64},\n",
    "                                         {'col': 'superstructure',\n",
    "                                          'data_type': dtype('O'),\n",
    "                                          'mapping': Stone, mud mortar        1\n",
    "Stone                    2\n",
    "RC, engineered           3\n",
    "Brick, cement mortar     4\n",
    "Adobe/mud                5\n",
    "Timber                   6\n",
    "RC, non-engineered       7\n",
    "Brick, mud mortar        8\n",
    "Stone, cement mortar     9\n",
    "Bamboo                  10\n",
    "Other                   11\n",
    "NaN                     -2\n",
    "dtype: int64}])),\n",
    "                ('decisiontreeclassifier',\n",
    "                 DecisionTreeClassifier(max_depth=6, random_state=42))])\n",
    "In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook.\n",
    "On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\n",
    "# Check your work\n",
    "assert isinstance(\n",
    "    model, Pipeline\n",
    "), f\"`model` should be a Pipeline, not type {type(model)}.\"\n",
    "assert isinstance(\n",
    "    model[0], OrdinalEncoder\n",
    "), f\"The first step in your Pipeline should be an OrdinalEncoder, not type {type(model[0])}.\"\n",
    "assert isinstance(\n",
    "    model[-1], DecisionTreeClassifier\n",
    "), f\"The last step in your Pipeline should be an DecisionTreeClassifier, not type {type(model[-1])}.\"\n",
    "check_is_fitted(model)\n",
    "VimeoVideo(\"665415153\", h=\"f0ec320955\", width=600)\n",
    "Task 4.3.7: Calculate the training and validation accuracy scores for your models.\n",
    "\n",
    "Calculate the accuracy score for a model in scikit-learn.\n",
    "Generate predictions using a trained model in scikit-learn.\n",
    "acc_train = accuracy_score(y_train, model.predict(X_train))\n",
    "acc_val = model.score(X_val, y_val)\n",
    "​\n",
    "print(\"Training Accuracy:\", round(acc_train, 2))\n",
    "print(\"Validation Accuracy:\", round(acc_val, 2))\n",
    "​\n",
    "\"\"\" the result showed that our test data 0.65 is did not beat our train data 0.98. we therefore \n",
    "    will do a hyper parameter tuning to correct this\n",
    "​\n",
    "\"\"\"\n",
    "Training Accuracy: 0.72\n",
    "Validation Accuracy: 0.72\n",
    "' the result showed that our test data 0.65 is did not beat our train data 0.98. we therefore \\n    will do a hyper parameter tuning to correct this\\n\\n'\n",
    "VimeoVideo(\"665415169\", h=\"44702fc696\", width=600)\n",
    "Task 4.3.8: Use the get_depth method on the DecisionTreeClassifier in your model to see how deep your tree grew during training.\n",
    "\n",
    "Access an object in a pipeline in scikit-learn.\n",
    "tree_depth = model.named_steps[\"decisiontreeclassifier\"].get_depth()\n",
    "print(\"Tree Depth:\", tree_depth)\n",
    "#our model is over fitting\n",
    "#our tree depth of 49 is way too much\n",
    "Tree Depth: 6\n",
    "VimeoVideo(\"665415186\", h=\"c4ce187170\", width=600)\n",
    "Task 4.3.9: Create a range of possible values for max_depth hyperparameter of your model's DecisionTreeClassifier. depth_hyperparams should range from 1 to 50 by steps of 2.\n",
    "\n",
    "What's an iterator?\n",
    "Create a range in Python.\n",
    "depth_hyperparams = range(1, 50, 2)\n",
    "# Check your work\n",
    "assert (\n",
    "    len(list(depth_hyperparams)) == 25\n",
    "), f\"`depth_hyperparams` should contain 25 items, not {len(list(depth_hyperparams))}.\"\n",
    "assert (\n",
    "    list(depth_hyperparams)[0] == 1\n",
    "), f\"`depth_hyperparams` should begin at 1, not {list(depth_hyperparams)[0]}.\"\n",
    "assert (\n",
    "    list(depth_hyperparams)[-1] == 49\n",
    "), f\"`depth_hyperparams` should end at 49, not {list(depth_hyperparams)[-1]}.\"\n",
    "VimeoVideo(\"665415198\", h=\"b4b85c3308\", width=600)\n",
    "Task 4.3.10: Complete the code below so that it trains a model for every max_depth in depth_hyperparams. Every time a new model is trained, the code should also calculate the training and validation accuracy scores and append them to the training_acc and validation_acc lists, respectively.\n",
    "\n",
    "Append an item to a list in Python.\n",
    "Create a pipeline in scikit-learn.\n",
    "Fit a model to training data in scikit-learn.\n",
    "Write a for loop in Python.\n",
    "# Create empty lists for training and validation accuracy scores\n",
    "training_acc = []\n",
    "validation_acc = []\n",
    "​\n",
    "for d in depth_hyperparams:\n",
    "    # Create model with `max_depth` of `d`\n",
    "    test_model = make_pipeline(\n",
    "        OrdinalEncoder(), \n",
    "        DecisionTreeClassifier(max_depth = d, random_state = 42)\n",
    "    )\n",
    "    # Fit model to training data\n",
    "    test_model.fit(X_train, y_train)\n",
    "    # Calculate training accuracy score and append to `training_acc`\n",
    "    training_acc.append(test_model.score(X_train, y_train))\n",
    "    # Calculate validation accuracy score and append to `training_acc`\n",
    "    validation_acc.append(test_model.score(X_val, y_val))\n",
    "​\n",
    "print(\"Training Accuracy Scores:\", training_acc[:3])\n",
    "print(\"Validation Accuracy Scores:\", validation_acc[:3])\n",
    "Training Accuracy Scores: [0.7071072484228174, 0.7117395332421582, 0.7162394670666608]\n",
    "Validation Accuracy Scores: [0.7088406564319746, 0.7132521616375508, 0.7166049055937886]\n",
    "# Check your work\n",
    "assert (\n",
    "    len(training_acc) == 25\n",
    "), f\"`training_acc` should contain 25 items, not {len(training_acc)}.\"\n",
    "assert (\n",
    "    len(validation_acc) == 25\n",
    "), f\"`validation_acc` should contain 25 items, not {len(validation_acc)}.\"\n",
    "VimeoVideo(\"665415236\", h=\"51d4be13fa\", width=600)\n",
    "Task 4.3.11: Create a visualization with two lines. The first line should plot the training_acc values as a function of depth_hyperparams, and the second should plot validation_acc as a function of depth_hyperparams. You x-axis should be labeled \"Max Depth\", and the y-axis \"Accuracy Score\". Also include a legend so that your audience can distinguish between the two lines.\n",
    "\n",
    "What's a line plot?\n",
    "Create a line plot in Matplotlib.\n",
    "# Plot `depth_hyperparams`, `training_acc`\n",
    "plt.plot(depth_hyperparams, training_acc, label=\"training\")\n",
    "plt.plot(depth_hyperparams, validation_acc, label=\"validation\")\n",
    "plt.xlabel(\"Max Depth\")\n",
    "plt.ylabel(\"Accuracy Score\")\n",
    "plt.legend();\n",
    "\n",
    "Evaluate\n",
    "VimeoVideo(\"665415255\", h=\"573e9cfd74\", width=600)\n",
    "Task 4.3.12: Based on your visualization, choose the max_depth value that leads to the best validation accuracy score. Then retrain your original model with that max_depth value. Lastly, check how your tuned model performs on your test set by calculating the test accuracy score below. Were you able to resolve the overfitting problem with this new max_depth?\n",
    "\n",
    "Calculate the accuracy score for a model in scikit-learn.\n",
    "Generate predictions using a trained model in scikit-learn.\n",
    "test_acc = model.score(X_test, y_test)\n",
    "print(\"Test Accuracy:\", round(test_acc, 2))\n",
    "Test Accuracy: 0.72\n",
    "Communicate\n",
    "VimeoVideo(\"665415275\", h=\"880366a826\", width=600)\n",
    "Task 4.3.13: Complete the code below to use the plot_tree function from scikit-learn to visualize the decision logic of your model.\n",
    "\n",
    "Plot a decision tree using scikit-learn.\n",
    "#type(X_train.columns.to_list())\n",
    "# Create larger figure\n",
    "fig, ax = plt.subplots(figsize=(25, 12))\n",
    "# Plot tree\n",
    "plot_tree(\n",
    "    decision_tree= model.named_steps[\"decisiontreeclassifier\"],\n",
    "    feature_names= X_train.columns.to_list(),\n",
    "    filled=True,  # Color leaf with class\n",
    "    rounded=True,  # Round leaf edges\n",
    "    proportion=True,  # Display proportion of classes in leaf\n",
    "    max_depth=3,  # Only display first 3 levels\n",
    "    fontsize=12,  # Enlarge font\n",
    "    ax=ax,  # Place in figure axis\n",
    ");\n",
    "\n",
    "VimeoVideo(\"665415304\", h=\"c7eeac08c9\", width=600)\n",
    "Task 4.3.14: Assign the feature names and importances of your model to the variables below. For the features, you can get them from the column names in your training set. For the importances, you access the feature_importances_ attribute of your model's DecisionTreeClassifier.\n",
    "\n",
    "Access an object in a pipeline in scikit-learn.\n",
    "features = X_train.columns\n",
    "importances = model.named_steps[\"decisiontreeclassifier\"].feature_importances_\n",
    "​\n",
    "print(\"Features:\", features[:3])\n",
    "print(\"Importances:\", importances[:3])\n",
    "Features: Index(['age_building', 'plinth_area_sq_ft', 'height_ft_pre_eq'], dtype='object')\n",
    "Importances: [0.03515085 0.04618639 0.08839161]\n",
    "# Check your work\n",
    "assert len(features) == 11, f\"`features` should contain 11 items, not {len(features)}.\"\n",
    "assert (\n",
    "    len(importances) == 11\n",
    "), f\"`importances` should contain 11 items, not {len(importances)}.\"\n",
    "Task 4.3.15: Create a pandas Series named feat_imp, where the index is features and the values are your importances. The Series should be sorted from smallest to largest importance.\n",
    "\n",
    "Create a Series in pandas.\n",
    "feat_imp = pd.Series(importances, index=features).sort_values()\n",
    "feat_imp.head()\n",
    "position                  0.000644\n",
    "plan_configuration        0.004847\n",
    "foundation_type           0.005206\n",
    "roof_type                 0.007620\n",
    "land_surface_condition    0.020759\n",
    "dtype: float64\n",
    "# Check your work\n",
    "assert isinstance(\n",
    "    feat_imp, pd.Series\n",
    "), f\"`feat_imp` should be a Series, not {type(feat_imp)}.\"\n",
    "assert feat_imp.shape == (\n",
    "    11,\n",
    "), f\"`feat_imp` should have shape (11,), not {feat_imp.shape}.\"\n",
    "VimeoVideo(\"665415316\", h=\"0dd9004477\", width=600)\n",
    "Task 4.3.16: Create a horizontal bar chart with all the features in feat_imp. Be sure to label your x-axis \"Gini Importance\".\n",
    "\n",
    "What's a bar chart?\n",
    "Create a bar chart using pandas.\n",
    "\n",
    "# Create horizontal bar chart\n",
    "feat_imp.plot(kind=\"barh\")\n",
    "plt.xlabel(\"Gini Importance\")\n",
    "plt.ylabel(\"Feature\");"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
