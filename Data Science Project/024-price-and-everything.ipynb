{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.4. Predicting Price with Size, Location, and Neighborhood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import warnings\n",
    "from glob import glob\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import wqet_grader\n",
    "from category_encoders import OneHotEncoder\n",
    "from IPython.display import VimeoVideo\n",
    "from ipywidgets import Dropdown, FloatSlider, IntSlider, interact\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LinearRegression, Ridge  # noqa F401\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.utils.validation import check_is_fitted\n",
    "\n",
    "warnings.simplefilter(action=\"ignore\", category=FutureWarning)\n",
    "wqet_grader.init(\"Project 2 Assessment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the final lesson for this project, we're going to try to use all the features in our dataset to improve our model. This means that we'll have to do a more careful cleaning of the dataset and consider some of the finer points of linear models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def wrangle(filepath):\n",
    "    # Read CSV file\n",
    "    df = pd.read_csv(filepath)\n",
    "\n",
    "    # Subset data: Apartments in \"Capital Federal\", less than 400,000\n",
    "    mask_ba = df[\"place_with_parent_names\"].str.contains(\"Capital Federal\")\n",
    "    mask_apt = df[\"property_type\"] == \"apartment\"\n",
    "    mask_price = df[\"price_aprox_usd\"] < 400_000\n",
    "    df = df[mask_ba & mask_apt & mask_price]\n",
    "\n",
    "    # Subset data: Remove outliers for \"surface_covered_in_m2\"\n",
    "    low, high = df[\"surface_covered_in_m2\"].quantile([0.1, 0.9])\n",
    "    mask_area = df[\"surface_covered_in_m2\"].between(low, high)\n",
    "    df = df[mask_area]\n",
    "\n",
    "    # Split \"lat-lon\" column\n",
    "    df[[\"lat\", \"lon\"]] = df[\"lat-lon\"].str.split(\",\", expand=True).astype(float)\n",
    "    df.drop(columns=\"lat-lon\", inplace=True)\n",
    "\n",
    "    # Get place name\n",
    "    df[\"neighborhood\"] = df[\"place_with_parent_names\"].str.split(\"|\", expand=True)[3]\n",
    "    df.drop(columns=\"place_with_parent_names\", inplace=True)\n",
    "    \n",
    "    #drop features with high null counts\n",
    "    df.drop(columns = [\"floor\", \"expenses\"], inplace =True)\n",
    "    \n",
    "    #Drop low and high-cardinality variables\n",
    "    df.drop(columns= [\"operation\", \"property_type\", \"currency\", \"properati_url\"], inplace = True)\n",
    "    \n",
    "    #drop leaky columns\n",
    "    df.drop(columns = [\n",
    "            'price',\n",
    "            'price_aprox_local_currency',\n",
    "            'price_per_m2',\n",
    "            'price_usd_per_m2'],\n",
    "            inplace = True)\n",
    "            \n",
    "    #drop multiconear columns\n",
    "    df.drop(columns=[\"surface_total_in_m2\", \"rooms\"], inplace= True)\n",
    "                     \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def wrangle(filepath):\n",
    "    # Read CSV file\n",
    "    df = pd.read_csv(filepath)\n",
    "\n",
    "    # Subset data: Apartments in \"Capital Federal\", less than 400,000\n",
    "    mask_ba = df[\"place_with_parent_names\"].str.contains(\"Capital Federal\")\n",
    "    mask_apt = df[\"property_type\"] == \"apartment\"\n",
    "    mask_price = df[\"price_aprox_usd\"] < 400_000\n",
    "    df = df[mask_ba & mask_apt & mask_price]\n",
    "\n",
    "    # Subset data: Remove outliers for \"surface_covered_in_m2\"\n",
    "    low, high = df[\"surface_covered_in_m2\"].quantile([0.1, 0.9])\n",
    "    mask_area = df[\"surface_covered_in_m2\"].between(low, high)\n",
    "    df = df[mask_area]\n",
    "\n",
    "    # Split \"lat-lon\" column\n",
    "    df[[\"lat\", \"lon\"]] = df[\"lat-lon\"].str.split(\",\", expand=True).astype(float)\n",
    "    df.drop(columns=\"lat-lon\", inplace=True)\n",
    "\n",
    "    # Get place name\n",
    "    df[\"neighborhood\"] = df[\"place_with_parent_names\"].str.split(\"|\", expand=True)[3]\n",
    "    df.drop(columns=\"place_with_parent_names\", inplace=True)\n",
    "    \n",
    "    #drop features with high null counts\n",
    "    df.drop(columns = [\"floor\", \"expenses\"], inplace =True)\n",
    "    \n",
    "    #Drop low and high-cardinality variables\n",
    "    df.drop(columns= [\"operation\", \"property_type\", \"currency\", \"properati_url\"], inplace = True)\n",
    "    \n",
    "    #drop leaky columns\n",
    "    df.drop(columns = [\n",
    "            'price',\n",
    "            'price_aprox_local_currency',\n",
    "            'price_per_m2',\n",
    "            'price_usd_per_m2'],\n",
    "            inplace = True)\n",
    "            \n",
    "    #drop multiconear columns\n",
    "    df.drop(columns=[\"surface_total_in_m2\", \"rooms\"], inplace= True)\n",
    "                     \n",
    "    return df\n",
    "def wrangle(filepath):\n",
    "    # Read CSV file\n",
    "    df = pd.read_csv(filepath)\n",
    "​\n",
    "    # Subset data: Apartments in \"Capital Federal\", less than 400,000\n",
    "    mask_ba = df[\"place_with_parent_names\"].str.contains(\"Capital Federal\")\n",
    "    mask_apt = df[\"property_type\"] == \"apartment\"\n",
    "    mask_price = df[\"price_aprox_usd\"] < 400_000\n",
    "    df = df[mask_ba & mask_apt & mask_price]\n",
    "​\n",
    "    # Subset data: Remove outliers for \"surface_covered_in_m2\"\n",
    "    low, high = df[\"surface_covered_in_m2\"].quantile([0.1, 0.9])\n",
    "    mask_area = df[\"surface_covered_in_m2\"].between(low, high)\n",
    "    df = df[mask_area]\n",
    "​\n",
    "    # Split \"lat-lon\" column\n",
    "    df[[\"lat\", \"lon\"]] = df[\"lat-lon\"].str.split(\",\", expand=True).astype(float)\n",
    "    df.drop(columns=\"lat-lon\", inplace=True)\n",
    "​\n",
    "    # Get place name\n",
    "    df[\"neighborhood\"] = df[\"place_with_parent_names\"].str.split(\"|\", expand=True)[3]\n",
    "    df.drop(columns=\"place_with_parent_names\", inplace=True)\n",
    "    \n",
    "    #drop features with high null counts\n",
    "    df.drop(columns = [\"floor\", \"expenses\"], inplace =True)\n",
    "    \n",
    "    #Drop low and high-cardinality variables\n",
    "    df.drop(columns= [\"operation\", \"property_type\", \"currency\", \"properati_url\"], inplace = True)\n",
    "    \n",
    "    #drop leaky columns\n",
    "    df.drop(columns = [\n",
    "            'price',\n",
    "            'price_aprox_local_currency',\n",
    "            'price_per_m2',\n",
    "            'price_usd_per_m2'],\n",
    "            inplace = True)\n",
    "            \n",
    "    #drop multiconear columns\n",
    "    df.drop(columns=[\"surface_total_in_m2\", \"rooms\"], inplace= True)\n",
    "                     \n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Let's begin by using what we've learned to load all our CSV files into a DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 2.4.1: Use glob to create a list that contains the filenames for all the Buenos Aires real estate CSV files in the data directory. Assign this list to the variable name files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "files = glob(\"data/buenos-aires-real-estate-*.csv\")\n",
    "files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check your work\n",
    "assert len(files) == 5, f\"`files` should contain 5 items, not {len(files)}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last time we put all our DataFrames into a list, we used a for loop. This time, we're going to use a more compact coding technique called a list comprehension.\n",
    "\n",
    "Task 2.4.2: Use your wrangle function in a list comprehension to create a list named frames. The list should contain the cleaned DataFrames for the filenames your collected in files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "frames = [wrangle(file) for file in files]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check your work\n",
    "assert len(frames) == 5, f\"`frames` should contain 5 items, not {len(frames)}\"\n",
    "assert all(\n",
    "    [isinstance(frame, pd.DataFrame) for frame in frames]\n",
    "), \"The items in `frames` should all be DataFrames.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last step: Combine the DataFrames in frames into a single df.\n",
    "\n",
    "Task 2.4.3: Use pd.concat to concatenate it items in frames into a single DataFrame df. Make sure you set the ignore_index argument to True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "frames = [wrangle(file) for file in files]\n",
    "df = pd.concat(frames, ignore_index=True)\n",
    "print(df.info())\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check your work\n",
    "assert len(df) == 6582, f\"`df` has the wrong number of rows: {len(df)}\"\n",
    "assert df.shape[1] <= 17, f\"`df` has too many columns: {df.shape[1]}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explore\n",
    "The first thing we need to consider when trying to use all the features df is missing values. While it's true you can impute missing values, there still needs to be enough data in a column to do a good imputation. A general rule is that, if more than half of the data in a column is missing, it's better to drop it then try imputing.\n",
    "\n",
    "Take a look at the output from df.info() above. Are there columns where more than half of the values are NaN? If so, those columns need to go!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#if a column is has >50% null values then it is better to drop it than to fillit\n",
    "df.isnull().sum()/len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 2.4.4: Modify your wrangle function to drop any columns that are more than half NaN values. Be sure to rerun all the cells above before you continue.\n",
    "\n",
    "Inspect a DataFrame using the shape, info, and head in pandas.\n",
    "Drop a column from a DataFrame using pandas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check your work\n",
    "assert len(df) == 6582, f\"`df` has the wrong number of rows: {len(df)}\"\n",
    "assert df.shape[1] <= 15, f\"`df` has too many columns: {df.shape[1]}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next thing we need to look out for are categorical columns with low or high cardinality. If there's only one category in a column, it won't provide any unique information to our model. At the other extreme, columns where nearly every row has its own category won't help our model in identifying useful trends in the data.\n",
    "\n",
    "Let's take a look at the cardinality of our features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Task 2.4.5: Calculate the number of unique values for each non-numeric feature in df.\n",
    "\n",
    "Subset a DataFrame's columns based on the column data types in pandas.\n",
    "Calculate summary statistics for a DataFrame or Series in pandas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df.select_dtypes(object).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df\u001b[38;5;241m.\u001b[39mselect_dtypes(\u001b[38;5;28mobject\u001b[39m)\u001b[38;5;241m.\u001b[39mnunique()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "df.select_dtypes(object).nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we can see that columns like \"operation\" have only one value in them, while every row in \"properati_url\" has a unique value. These are clear examples of high- and low-cardinality features that we shouldn't include in our model.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Task 2.4.6: Modify your wrangle function to drop high- and low-cardinality categorical features.\n",
    "\n",
    "Be sure to rerun all the cells above before you continue.\n",
    "\n",
    "What are high- and low-cardinality features?\n",
    "Drop a column from a DataFrame using pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check your work\n",
    "assert len(df) == 6582, f\"`df` has the wrong number of rows: {len(df)}\"\n",
    "assert df.shape[1] <= 11, f\"`df` has too many columns: {df.shape[1]}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's also important for us to drop any columns that would constitute leakage, that is, features that were created using our target or that would give our model information that it won't have access to when it's deployed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Task 2.4.7: Modify your wrangle function to drop any features that would constitute leakage.\n",
    "\n",
    "Be sure to rerun all the cells above before you continue.\n",
    "\n",
    "What's leakage?\n",
    "Drop a column from a DataFrame using pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check your work\n",
    "assert len(df) == 6582, f\"`df` has the wrong number of rows: {len(df)}\"\n",
    "assert df.shape[1] <= 7, f\"`df` has too many columns: {df.shape[1]}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the last issue we need to keep an eye out for is multicollinearity, that is, features in our feature matrix that are highly correlated with each other. A good way to detect this is to use a heatmap. Let's make one!\n",
    "\n",
    "\n",
    "Task 2.4.8: Plot a correlation heatmap of the remaining numerical features in df. Since \"price_aprox_usd\" will be your target, you don't need to include it in your heatmap.\n",
    "\n",
    "What's a heatmap?\n",
    "Create a correlation matrix in pandas.\n",
    "Create a heatmap in seaborn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = df.select_dtypes(\"number\").drop(columns=\"price_aprox_usd\").corr()\n",
    "sns.heatmap(corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 2.4.9: Modify your wrangle function to remove columns so that there are no strongly correlated features in your feature matrix.\n",
    "\n",
    "Be sure to rerun all the cells above before you continue.\n",
    "\n",
    "What's multicollinearity?\n",
    "Drop a column from a DataFrame using pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check your work\n",
    "assert len(df) == 6582, f\"`df` has the wrong number of rows: {len(df)}\"\n",
    "assert df.shape[1] == 5, f\"`df` has the wrong number of columns: {df.shape[1]}\"\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Done! It looks like we're going to use the four features we've used in our previous models but, this time, we're going to combine them.\n",
    "\n",
    "Split Data¶\n",
    "Task 2.4.10: Create your feature matrix X_train and target vector y_train. Your target is \"price_aprox_usd\". Your features should be all the columns that remain in the DataFrame you cleaned above.\n",
    "\n",
    "What's a feature matrix?\n",
    "What's a target vector?\n",
    "Subset a DataFrame by selecting one or more columns in pandas.\n",
    "Select a Series from a DataFrame in pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = \"price_aprox_usd\"\n",
    "features =[\"surface_covered_in_m2\", \"lat\", \"lon\", \"neighborhood\"]\n",
    "y_train = df[target]\n",
    "X_train = df[features]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check your work\n",
    "assert X_train.shape == (6582, 4), f\"`X_train` is the wrong size: {X_train.shape}.\"\n",
    "assert y_train.shape == (6582,), f\"`y_train` is the wrong size: {y_train.shape}.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build Model\n",
    "Baseline¶\n",
    "\n",
    "\n",
    "\n",
    "Task 2.4.11: Calculate the baseline mean absolute error for your model.\n",
    "\n",
    "Calculate summary statistics for a DataFrame or Series in pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_mean = y_train.mean()\n",
    "y_pred_baseline = [y_mean] * len(y_train)\n",
    "print(\"Mean apt price:\", round(y_mean, 2))\n",
    "\n",
    "print(\"Baseline MAE:\", round(mean_absolute_error(y_train, y_pred_baseline), 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iterate\n",
    "Task 2.4.12: Create a pipeline named model that contains a OneHotEncoder, SimpleImputer, and Ridge predictor.\n",
    "\n",
    "What's imputation?\n",
    "What's one-hot encoding?\n",
    "What's a pipeline?\n",
    "Create a pipeline in scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#instantiate\n",
    "ohe = OneHotEncoder(use_cat_names = True)\n",
    "\n",
    "#Fit\n",
    "ohe.fit(X_train)\n",
    "\n",
    "#Transform\n",
    "XT_train = ohe.transform(X_train)\n",
    "\n",
    "print(XT_train.shape)\n",
    "XT_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imputer= SimpleImputer()\n",
    "#imputer.fit(X_train=[\"surface_covered_in_m2\", \"lat\", \"lon\"])\n",
    "#XT_train = imputer.transform(X_train)\n",
    "#pd.DataFrame(XT_train, columns=X_train.columns).info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = make_pipeline(\n",
    "    OneHotEncoder(use_cat_names=True),\n",
    "    SimpleImputer(),\n",
    "    Ridge()\n",
    ")\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check your work\n",
    "check_is_fitted(model[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate\n",
    "VimeoVideo(\"656849505\", h=\"f153a4f005\", width=600)\n",
    "Task 2.4.13: Calculate the training mean absolute error for your predictions as compared to the true targets in y_train.\n",
    "\n",
    "Generate predictions using a trained model in scikit-learn.\n",
    "Calculate the mean absolute error for a list of predictions in scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_training = model.predict(X_train)\n",
    "print(\"Training MAE:\", mean_absolute_error(y_train, y_pred_training))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Task 2.4.14: Run the code below to import your test data buenos-aires-test-features.csv into a DataFrame and generate a list of predictions using your model. Then run the following cell to submit your predictions to the grader.\n",
    "\n",
    "What's generalizability?\n",
    "Generate predictions using a trained model in scikit-learn.\n",
    "Calculate the mean absolute error for a list of predictions in scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = pd.read_csv(\"data/buenos-aires-test-features.csv\")\n",
    "y_pred_test = pd.Series(model.predict(X_test))\n",
    "y_pred_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Communicate Results¶\n",
    "For this lesson, we've relied on equations and visualizations for communication about our model. In many data science projects, however, communication means giving stakeholders tools they can use to deploy a model — in other words, use it in action. So let's look at two ways you might deploy this model.\n",
    "\n",
    "One thing you might be asked to do it wrap your model in a function so that a programmer can provide inputs and then receive a prediction as output.\n",
    "\n",
    "\n",
    "Task 2.4.15: Create a function make_prediction that takes four arguments (area, lat, lon, and neighborhood) and returns your model's prediction for an apartment price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_prediction(area, lat, lon, neighborhood):\n",
    "    data = {\n",
    "    \"surface_covered_in_m2\": area,\n",
    "    \"lat\": lat,\n",
    "    \"lon\": lon,\n",
    "    \"neighborhood\": neighborhood\n",
    "    }\n",
    "    df = pd.DataFrame(data, index=[0])\n",
    "    prediction = model.predict(df).round(2)[0]\n",
    "    return f\"Predicted apartment price: ${prediction}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another type of deployment is creating and interactive dashboard, where a user can supply values and receive a prediction. Let's create one using Jupyter Widgets.WQU WorldQuant University Applied Data Science Lab \n",
    "\n",
    "\n",
    "Task 2.4.16: Add your make_prediction to the interact widget below, run the cell, and then adjust the widget to see how predicted apartment price changes.\n",
    "\n",
    "Create an interact function in Jupyter Widgets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interact(\n",
    "    make_prediction,\n",
    "    area=IntSlider(\n",
    "        min=X_train[\"surface_covered_in_m2\"].min(),\n",
    "        max=X_train[\"surface_covered_in_m2\"].max(),\n",
    "        value=X_train[\"surface_covered_in_m2\"].mean(),\n",
    "    ),\n",
    "    lat=FloatSlider(\n",
    "        min=X_train[\"lat\"].min(),\n",
    "        max=X_train[\"lat\"].max(),\n",
    "        step=0.01,\n",
    "        value=X_train[\"lat\"].mean(),\n",
    "    ),\n",
    "    lon=FloatSlider(\n",
    "        min=X_train[\"lon\"].min(),\n",
    "        max=X_train[\"lon\"].max(),\n",
    "        step=0.01,\n",
    "        value=X_train[\"lon\"].mean(),\n",
    "    ),\n",
    "    neighborhood=Dropdown(options=sorted(X_train[\"neighborhood\"].unique())),\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great work! You may have noticed that there a lots of ways to improve this dashboard. For instance, a user can select a neighborhood and then supply latitude-longitude coordinates that aren't in that neighborhood. It would also be helpful to include a visualization like a map. Regardless, this is a great first step towards creating dynamic dashboards that turn your model from a complicated abstraction to a concrete tool that anyone can access. One of the most important parts of data science projects is creating products that people can use to make their work or lives easier."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
