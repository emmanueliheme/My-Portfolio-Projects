{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "6.2. Clustering with Two Features\n",
    "\n",
    "In the previous lesson, you explored data from the Survey of Consumer Finances (SCF), paying special attention to households that have been turned down for credit or feared being denied credit. In this lesson, we'll build a model to segment those households into distinct clusters, and examine the differences between those clusters.\n",
    "\n",
    "​\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import wqet_grader\n",
    "from IPython.display import VimeoVideo\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.utils.validation import check_is_fitted\n",
    "from teaching_tools.widgets import ClusterWidget, SCFClusterWidget\n",
    "​\n",
    "wqet_grader.init(\"Project 6 Assessment\")\n",
    "​\n",
    "VimeoVideo(\"713919442\", h=\"7b4cbc1495\", width=600)\n",
    "Prepare Data\n",
    "unsupervised learning like clustering there is no target vector during unlike supervised learnng eg. regression, classification\n",
    "\n",
    "Import\n",
    "Just like always, we need to begin by bringing our data into the project. We spent some time in the previous lesson working with a subset of the larger SCF dataset called \"TURNFEAR\". Let's start with that.\n",
    "\n",
    "VimeoVideo(\"713919411\", h=\"fd4fae4013\", width=600)\n",
    "Task 6.2.1: Create a wrangle function that takes a path of a CSV file as input, reads the file into a DataFrame, subsets the data to households that have been turned down for credit or feared being denied credit in the past 5 years (see \"TURNFEAR\"), and returns the subset DataFrame.\n",
    "\n",
    "Write a function in Python.\n",
    "Subset a DataFrame by selecting one or more columns in pandas.\n",
    "def wrangle(filepath):\n",
    "    df = pd.read_csv(filepath)\n",
    "    mask = df[\"TURNFEAR\"] == 1\n",
    "    df = df[mask]\n",
    "    return df\n",
    "And now that we've got that taken care of, we'll import the data and see what we've got.\n",
    "\n",
    "Task 6.2.2: Use your wrangle function to read the file SCFP2019.csv.gz into a DataFrame named df.\n",
    "\n",
    "Read a CSV file into a DataFrame using pandas.\n",
    "df = wrangle(\"data/SCFP2019.csv.gz\")\n",
    "​\n",
    "print(\"df type:\", type(df))\n",
    "print(\"df shape:\", df.shape)\n",
    "df.head()\n",
    "df type: <class 'pandas.core.frame.DataFrame'>\n",
    "df shape: (4623, 351)\n",
    "YY1\tY1\tWGT\tHHSEX\tAGE\tAGECL\tEDUC\tEDCL\tMARRIED\tKIDS\t...\tNWCAT\tINCCAT\tASSETCAT\tNINCCAT\tNINC2CAT\tNWPCTLECAT\tINCPCTLECAT\tNINCPCTLECAT\tINCQRTCAT\tNINCQRTCAT\n",
    "5\t2\t21\t3790.476607\t1\t50\t3\t8\t2\t1\t3\t...\t1\t2\t1\t2\t1\t1\t4\t4\t2\t2\n",
    "6\t2\t22\t3798.868505\t1\t50\t3\t8\t2\t1\t3\t...\t1\t2\t1\t2\t1\t1\t4\t3\t2\t2\n",
    "7\t2\t23\t3799.468393\t1\t50\t3\t8\t2\t1\t3\t...\t1\t2\t1\t2\t1\t1\t4\t4\t2\t2\n",
    "8\t2\t24\t3788.076005\t1\t50\t3\t8\t2\t1\t3\t...\t1\t2\t1\t2\t1\t1\t4\t4\t2\t2\n",
    "9\t2\t25\t3793.066589\t1\t50\t3\t8\t2\t1\t3\t...\t1\t2\t1\t2\t1\t1\t4\t4\t2\t2\n",
    "5 rows × 351 columns\n",
    "\n",
    "Explore\n",
    "We looked at a lot of different features of the \"TURNFEAR\" subset in the last lesson, and the last thing we looked at was the relationship between real estate and debt. To refresh our memory on what that relationship looked like, let's make that graph again.\n",
    "\n",
    "VimeoVideo(\"713919351\", h=\"55dc979d55\", width=600)\n",
    "Task 6.2.3: Create a scatter plot of that shows the total value of primary residence of a household (\"HOUSES\") as a function of the total value of household debt (\"DEBT\"). Be sure to label your x-axis as \"Household Debt\", your y-axis as \"Home Value\", and use the title \"Credit Fearful: Home Value vs. Household Debt\".\n",
    "\n",
    "What's a scatter plot?\n",
    "Create a scatter plot using seaborn.\n",
    "(df[\"DEBT\"])/1e6\n",
    "5        0.0122\n",
    "6        0.0126\n",
    "7        0.0153\n",
    "8        0.0141\n",
    "9        0.0154\n",
    "          ...  \n",
    "28865    0.0104\n",
    "28866    0.0104\n",
    "28867    0.0104\n",
    "28868    0.0104\n",
    "28869    0.0104\n",
    "Name: DEBT, Length: 4623, dtype: float64\n",
    "# Plot \"HOUSES\" vs \"DEBT\"\n",
    "sns.scatterplot(x=(df[\"DEBT\"])/1e6, y=df[\"HOUSES\"]/1e6)\n",
    "plt.xlabel(\"Household Debt [$1M]\")\n",
    "plt.ylabel(\"Home Value [$1M]\")\n",
    "plt.title(\"Credit Fearful: Home Value vs. Household Debt\");\n",
    "\n",
    "Remember that graph and its clusters? Let's get a little deeper into it.\n",
    "\n",
    "Split\n",
    "We need to split our data, but we're not going to need target vector or a test set this time around. That's because the model we'll be building involves unsupervised learning. It's called unsupervised because the model doesn't try to map input to a st of labels or targets that already exist. It's kind of like how humans learn new skills, in that we don't always have models to copy. Sometimes, we just try out something and see what happens. Keep in mind that this doesn't make these models any less useful, it just makes them different.\n",
    "\n",
    "So, keeping that in mind, let's do the split.\n",
    "\n",
    "VimeoVideo(\"713919336\", h=\"775867f48a\", width=600)\n",
    "Task 6.2.4: Create the feature matrix X. It should contain two features only: \"DEBT\" and \"HOUSES\".\n",
    "\n",
    "What's a feature matrix?\n",
    "Subset a DataFrame by selecting one or more columns in pandas.\n",
    "X = df[[\"DEBT\", \"HOUSES\"]]\n",
    "​\n",
    "print(\"X type:\", type(X))\n",
    "print(\"X shape:\", X.shape)\n",
    "X.head()\n",
    "X type: <class 'pandas.core.frame.DataFrame'>\n",
    "X shape: (4623, 2)\n",
    "DEBT\tHOUSES\n",
    "5\t12200.0\t0.0\n",
    "6\t12600.0\t0.0\n",
    "7\t15300.0\t0.0\n",
    "8\t14100.0\t0.0\n",
    "9\t15400.0\t0.0\n",
    "Build Model\n",
    "Before we start building the model, let's take a second to talk about something called KMeans.\n",
    "\n",
    "Take another look at the scatter plot we made at the beginning of this lesson. Remember how the datapoints form little clusters? It turns out we can use an algorithm that partitions the dataset into smaller groups.\n",
    "\n",
    "Let's take a look at how those things work together.\n",
    "\n",
    "VimeoVideo(\"713919214\", h=\"028502efe7\", width=600)\n",
    "Task 6.2.5: Run the cell below to display the ClusterWidget.\n",
    "\n",
    "What's a centroid?\n",
    "What's a cluster?\n",
    "cw = ClusterWidget(n_clusters=3)\n",
    "cw.show()\n",
    "VBox(children=(IntSlider(value=0, continuous_update=False, description='Step:', max=10), Output(layout=Layout(…\n",
    "Take a second and run slowly through all the positions on the slider. At the first position, there's whole bunch of gray datapoints, and if you look carefully, you'll see there are also three stars. Those stars are the centroids. At first, their position is set randomly. If you move the slider one more position to the right, you'll see all the gray points change colors that correspond to three clusters.\n",
    "\n",
    "Since a centroid represents the mean value of all the data in the cluster, we would expect it to fall in the center of whatever cluster it's in. That's what will happen if you move the slider one more position to the right. See how the centroids moved?\n",
    "\n",
    "Aha! But since they moved, the datapoints might not be in the right clusters anymore. Move the slider again, and you'll see the data points redistribute themselves to better reflect the new position of the centroids. The new clusters mean that the centroids also need to move, which will lead to the clusters changing again, and so on, until all the datapoints end up in the right cluster with a centroid that reflects the mean value of all those points.\n",
    "\n",
    "Let's see what happens when we try the same with our \"DEBT\" and \"HOUSES\" data.\n",
    "\n",
    "VimeoVideo(\"713919177\", h=\"102616b1c3\", width=600)\n",
    "Task 6.2.6: Run the cell below to display the SCFClusterWidget.\n",
    "\n",
    "scfc = SCFClusterWidget(x=df[\"DEBT\"], y=df[\"HOUSES\"], n_clusters=3)\n",
    "scfc.show()\n",
    "VBox(children=(IntSlider(value=0, continuous_update=False, description='Step:', max=10), Output(layout=Layout(…\n",
    "Iterate\n",
    "Now that you've had a chance to play around with the process a little bit, let's get into how to build a model that does the same thing.\n",
    "\n",
    "VimeoVideo(\"713919157\", h=\"0b2c3c95f2\", width=600)\n",
    "Task 6.2.7: Build a KMeans model, assign it to the variable name model, and fit it to the training data X.\n",
    "\n",
    "What's k-means clustering?\n",
    "Fit a model to training data in scikit-learn.\n",
    "Tip: The k-means clustering algorithm relies on random processes, so don't forget to set a random_state for all your models in this lesson.\n",
    "# Build model\n",
    "model = KMeans(n_clusters=3, random_state=42)\n",
    "print(\"model type:\", type(model))\n",
    "​\n",
    "# Fit model to data\n",
    "model.fit(X)\n",
    "# Assert that model has been fit to data\n",
    "check_is_fitted(model)\n",
    "model type: <class 'sklearn.cluster._kmeans.KMeans'>\n",
    "/opt/conda/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
    "  super()._check_params_vs_input(X, default_n_init=10)\n",
    "And there it is. 42 datapoints spread across three clusters. Let's grab the labels that the model has assigned to the data points so we can start making a new visualization.\n",
    "\n",
    "VimeoVideo(\"713919137\", h=\"7eafe805ff\", width=600)\n",
    "Task 6.2.8: Extract the labels that your model created during training and assign them to the variable labels.\n",
    "\n",
    "Access an object in a pipeline in scikit-learn.\n",
    "labels = model.labels_\n",
    "print(\"labels type:\", type(labels))\n",
    "print(\"labels shape:\", labels.shape)\n",
    "labels[:10]\n",
    "labels type: <class 'numpy.ndarray'>\n",
    "labels shape: (4623,)\n",
    "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int32)\n",
    "Using the labels we just extracted, let's recreate the scatter plot from before, this time we'll color each point according to the cluster to which the model assigned it.\n",
    "\n",
    "VimeoVideo(\"713919104\", h=\"2f6d4285f1\", width=600)\n",
    "Task 6.2.9: Recreate the \"Home Value vs. Household Debt\" scatter plot you made above, but with two changes. First, use seaborn to create the plot. Second, pass your labels to the hue argument, and set the palette argument to \"deep\".\n",
    "\n",
    "What's a scatter plot?\n",
    "Create a scatter plot using seaborn.\n",
    "# Plot \"HOUSES\" vs \"DEBT\" with hue=label\n",
    "sns.scatterplot(\n",
    "    x=df[\"DEBT\"]/1e6,\n",
    "    y=df[\"HOUSES\"]/1e6,\n",
    "    hue=labels,\n",
    "    palette=\"deep\"\n",
    ")\n",
    "plt.xlabel(\"Household Debt [$1M]\")\n",
    "plt.ylabel(\"Home Value [$1M]\")\n",
    "plt.title(\"Credit Fearful: Home Value vs. Household Debt\");\n",
    "\n",
    "Nice! Each cluster has its own color. The centroids are still missing, so let's pull those out.\n",
    "\n",
    "VimeoVideo(\"713919087\", h=\"9b8635c9a8\", width=600)\n",
    "Task 6.2.10: Extract the centroids that your model created during training, and assign them to the variable centroids.\n",
    "\n",
    "What's a centroid?\n",
    "centroids = model.cluster_centers_\n",
    "print(\"centroids type:\", type(centroids))\n",
    "print(\"centroids shape:\", centroids.shape)\n",
    "centroids\n",
    "centroids type: <class 'numpy.ndarray'>\n",
    "centroids shape: (3, 2)\n",
    "array([[   91017.57766674,   116150.29328698],\n",
    "       [18384100.        , 34484000.        ],\n",
    "       [ 5065800.        , 11666666.66666667]])\n",
    "Let's add the centroids to the graph.\n",
    "\n",
    "VimeoVideo(\"713919002\", h=\"08cba14f6b\", width=600)\n",
    "Task 6.2.11: Recreate the seaborn \"Home Value vs. Household Debt\" scatter plot you just made, but with one difference: Add the centroids to the plot. Be sure to set the centroids color to \"gray\".\n",
    "\n",
    "What's a scatter plot?\n",
    "Create a scatter plot using seaborn.\n",
    "# Plot \"HOUSES\" vs \"DEBT\", add centroids\n",
    "sns.scatterplot(\n",
    "    x=df[\"DEBT\"]/1e6,\n",
    "    y=df[\"HOUSES\"]/1e6,\n",
    "    hue=labels,\n",
    "    palette=\"deep\"\n",
    ")\n",
    "plt.scatter(\n",
    "    x=centroids[:, 0]/1e6,\n",
    "    y=centroids[:, 1]/1e6,\n",
    "    color=\"gray\",\n",
    "    marker=\"*\",\n",
    "    s=150\n",
    "    \n",
    ")\n",
    "plt.xlabel(\"Household Debt [$1M]\")\n",
    "plt.ylabel(\"Home Value [$1M]\")\n",
    "plt.title(\"Credit Fearful: Home Value vs. Household Debt\");\n",
    "\n",
    "That looks great, but let's not pat ourselves on the back just yet. Even though our graph makes it look like the clusters are correctly assigned but, as data scientists, we need a numerical evaluation. The data we're using is pretty clear-cut, but if things were a little more muddled, we'd want to run some calculations to make sure we got everything right.\n",
    "\n",
    "There are two metrics that we'll use to evaluate our clusters. We'll start with inertia, which measure the distance between the points within the same cluster.\n",
    "\n",
    "inertia - within clusters sum of squares\n",
    "\n",
    "VimeoVideo(\"713918749\", h=\"bfc741b1e7\", width=600)\n",
    "Question: What do those double bars in the equation mean?\n",
    "\n",
    "Answer: It's the L2 norm, that is, the non-negative Euclidean distance between each datapoint and its centroid. In Python, it would be something like sqrt((x1-c)**2 + (x2-c)**2) + ...).\n",
    "\n",
    "Many thanks to Aghogho Esuoma Monorien for his comment in the forum! 🙏\n",
    "\n",
    "Task 6.2.12: Extract the inertia for your model and assign it to the variable inertia.\n",
    "\n",
    "What's inertia?\n",
    "Access an object in a pipeline in scikit-learn.\n",
    "Calculate the inertia for a model in scikit-learn.\n",
    "inertia = model.inertia_\n",
    "print(\"inertia type:\", type(inertia))\n",
    "print(\"Inertia (3 clusters):\", inertia)\n",
    "inertia type: <class 'float'>\n",
    "Inertia (3 clusters): 939554010797059.4\n",
    "The \"best\" inertia is 0, and our score is pretty far from that. Does that mean our model is \"bad?\" Not necessarily. Inertia is a measurement of distance (like mean absolute error from Project 2). This means that the unit of measurement for inertia depends on the unit of measurement of our x- and y-axes. And since \"DEBT\" and \"HOUSES\" are measured in tens of millions of dollars, it's not surprising that inertia is so large.\n",
    "\n",
    "However, it would be helpful to have metric that was easier to interpret, and that's where silhouette score comes in. Silhouette score measures the distance between different clusters. It ranges from -1 (the worst) to 1 (the best), so it's easier to interpret than inertia. WQU WorldQuant University Applied Data Science Lab QQQQ\n",
    "\n",
    "VimeoVideo(\"713918501\", h=\"0462c4784a\", width=600)\n",
    "silhouette score measures distance between clusters\n",
    "\n",
    "Task 6.2.13: Calculate the silhouette score for your model and assign it to the variable ss.\n",
    "\n",
    "What's silhouette score?\n",
    "Calculate the silhouette score for a model in scikit-learn.\n",
    "ss = silhouette_score(X, model.labels_)\n",
    "print(\"ss type:\", type(ss))\n",
    "print(\"Silhouette Score (3 clusters):\", ss)\n",
    "ss type: <class 'numpy.float64'>\n",
    "Silhouette Score (3 clusters): 0.9768842462944348\n",
    "Outstanding! 0.976 is pretty close to 1, so our model has done a good job at identifying 3 clusters that are far away from each other.\n",
    "\n",
    "It's important to remember that these performance metrics are the result of the number of clusters we told our model to create. In unsupervised learning, the number of clusters is hyperparameter that you set before training your model. So what would happen if we change the number of clusters? Will it lead to better performance? Let's try!\n",
    "\n",
    "VimeoVideo(\"713918420\", h=\"e16f3735c7\", width=600)\n",
    "Task 6.2.14: Use a for loop to build and train a K-Means model where n_clusters ranges from 2 to 12 (inclusive). Each time a model is trained, calculate the inertia and add it to the list inertia_errors, then calculate the silhouette score and add it to the list silhouette_scores.\n",
    "\n",
    "Write a for loop in Python.\n",
    "Calculate the inertia for a model in scikit-learn.\n",
    "Calculate the silhouette score for a model in scikit-learn.\n",
    "n_clusters = range(2, 13)\n",
    "inertia_errors = []\n",
    "silhouette_scores = []\n",
    "​\n",
    "# Add `for` loop to train model and calculate inertia, silhouette score.\n",
    "for k in n_clusters:\n",
    "    # Buil model\n",
    "    model=KMeans(n_clusters=k, random_state=42)\n",
    "    #Train model\n",
    "    model.fit(X)\n",
    "    #Calculate inertia\n",
    "    inertia_errors.append(model.inertia_)\n",
    "    #calculate ss\n",
    "    silhouette_scores.append(silhouette_score(X, model.labels_))\n",
    "    \n",
    "    \n",
    "    \n",
    "​\n",
    "print(\"inertia_errors type:\", type(inertia_errors))\n",
    "print(\"inertia_errors len:\", len(inertia_errors))\n",
    "print(\"Inertia:\", inertia_errors)\n",
    "print()\n",
    "print(\"silhouette_scores type:\", type(silhouette_scores))\n",
    "print(\"silhouette_scores len:\", len(silhouette_scores))\n",
    "print(\"Silhouette Scores:\", silhouette_scores)\n",
    "/opt/conda/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
    "  super()._check_params_vs_input(X, default_n_init=10)\n",
    "/opt/conda/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
    "  super()._check_params_vs_input(X, default_n_init=10)\n",
    "/opt/conda/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
    "  super()._check_params_vs_input(X, default_n_init=10)\n",
    "/opt/conda/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
    "  super()._check_params_vs_input(X, default_n_init=10)\n",
    "/opt/conda/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
    "  super()._check_params_vs_input(X, default_n_init=10)\n",
    "/opt/conda/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
    "  super()._check_params_vs_input(X, default_n_init=10)\n",
    "/opt/conda/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
    "  super()._check_params_vs_input(X, default_n_init=10)\n",
    "/opt/conda/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
    "  super()._check_params_vs_input(X, default_n_init=10)\n",
    "/opt/conda/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
    "  super()._check_params_vs_input(X, default_n_init=10)\n",
    "/opt/conda/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
    "  super()._check_params_vs_input(X, default_n_init=10)\n",
    "/opt/conda/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
    "  super()._check_params_vs_input(X, default_n_init=10)\n",
    "inertia_errors type: <class 'list'>\n",
    "inertia_errors len: 11\n",
    "Inertia: [3018038313336857.5, 939554010797059.4, 546098841715646.25, 309313172681861.4, 235250007188435.38, 182185545995311.7, 150727950872604.22, 114321995931021.89, 100488983856739.94, 86227397125225.02, 73193859398329.2]\n",
    "\n",
    "silhouette_scores type: <class 'list'>\n",
    "silhouette_scores len: 11\n",
    "Silhouette Scores: [0.9855099957519555, 0.9768842462944348, 0.9490311483406091, 0.839669623678179, 0.7526801280714244, 0.7277940458463407, 0.7256332651512161, 0.7335125606476427, 0.7313509140373811, 0.6950363232867054, 0.6964839563551604]\n",
    "Now that we have both performance metrics for several different settings of n_clusters, let's make some line plots to see the relationship between the number of clusters in a model and its inertia and silhouette scores.\n",
    "\n",
    "VimeoVideo(\"713918224\", h=\"32ff34ffa1\", width=600)\n",
    "Task 6.2.15: Create a line plot that shows the values of inertia_errors as a function of n_clusters. Be sure to label your x-axis \"Number of Clusters\", your y-axis \"Inertia\", and use the title \"K-Means Model: Inertia vs Number of Clusters\".\n",
    "\n",
    "Create a line plot in Matplotlib.\n",
    "# Plot `inertia_errors` by `n_clusters`\n",
    "plt.plot(n_clusters, inertia_errors)\n",
    "plt.xlabel(\"Number of clusters(k)\")\n",
    "plt.ylabel(\"Inertia\")\n",
    "plt.title(\"K-Means Model: Inertia vs Number of Clusters\");\n",
    "\n",
    "What we're seeing here is that, as the number of clusters increases, inertia goes down. In fact, we could get inertia to 0 if we told our model to make 4,623 clusters (the same number of observations in X), but those clusters wouldn't be helpful to us.\n",
    "\n",
    "The trick with choosing the right number of clusters is to look for the \"bend in the elbow\" for this plot. In other words, we want to pick the point where the drop in inertia becomes less dramatic and the line begins to flatten out. In this case, it looks like the sweet spot is 4 or 5.\n",
    "\n",
    "Let's see what the silhouette score looks like.\n",
    "\n",
    "VimeoVideo(\"713918153\", h=\"3f3a1312d2\", width=600)\n",
    "Task 6.2.16: Create a line plot that shows the values of silhouette_scores as a function of n_clusters. Be sure to label your x-axis \"Number of Clusters\", your y-axis \"Silhouette Score\", and use the title \"K-Means Model: Silhouette Score vs Number of Clusters\".\n",
    "\n",
    "Create a line plot in Matplotlib.\n",
    "# Plot `silhouette_scores` vs `n_clusters`\n",
    "plt.plot(n_clusters, silhouette_scores)\n",
    "plt.xlabel(\"Number of clusters\")\n",
    "plt.ylabel(\"Silhouette Score\")\n",
    "plt.title(\"K-Means Model: Silhouette Score  vs Number of Clusters\");\n",
    "\n",
    "Note that, in contrast to our inertia plot, bigger is better. So we're not looking for a \"bend in the elbow\" but rather a number of clusters for which the silhouette score still remains high. We can see that silhouette score drops drastically beyond 4 clusters. Given this and what we saw in the inertia plot, it looks like the optimal number of clusters is 4.\n",
    "\n",
    "Now that we've decided on the final number of clusters, let's build a final model.\n",
    "\n",
    "VimeoVideo(\"713918108\", h=\"e6aa88569e\", width=600)\n",
    "Task 6.2.17: Build and train a new k-means model named final_model. Use the information you gained from the two plots above to set an appropriate value for the n_clusters argument. Once you've built and trained your model, submit it to the grader for evaluation.\n",
    "\n",
    "Fit a model to training data in scikit-learn.\n",
    "# Build model\n",
    "final_model = KMeans(n_clusters=4, random_state=42)\n",
    "print(\"final_model type:\", type(final_model))\n",
    "​\n",
    "# Fit model to data\n",
    "​\n",
    "final_model.fit(X)\n",
    "# Assert that model has been fit to data\n",
    "check_is_fitted(final_model)\n",
    "final_model type: <class 'sklearn.cluster._kmeans.KMeans'>\n",
    "/opt/conda/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
    "  super()._check_params_vs_input(X, default_n_init=10)\n",
    "​\n",
    "wqet_grader.grade(\"Project 6 Assessment\", \"Task 6.2.17\", final_model)\n",
    "Party time! 🎉🎉🎉\n",
    "\n",
    "Score: 1\n",
    "\n",
    "(In case you're wondering, we don't need an Evaluate section in this notebook because we don't have any test data to evaluate our model with.)\n",
    "\n",
    "Communicate\n",
    "VimeoVideo(\"713918073\", h=\"3929b58011\", width=600)\n",
    "Task 6.2.18: Create one last \"Home Value vs. Household Debt\" scatter plot that shows the clusters that your final_model has assigned to the training data.\n",
    "\n",
    "What's a scatter plot?\n",
    "Create a scatter plot using Matplotlib.\n",
    "# Plot \"HOUSES\" vs \"DEBT\" with final_model labels\n",
    "sns.scatterplot(\n",
    "    x=df[\"DEBT\"]/1e6,\n",
    "    y=df[\"HOUSES\"]/1e6,\n",
    "    hue=final_model.labels_,\n",
    "    palette=\"deep\"\n",
    "​\n",
    ")\n",
    "plt.xlabel(\"Household Debt [$1M]\")\n",
    "plt.ylabel(\"Home Value [$1M]\")\n",
    "plt.title(\"Credit Fearful: Home Value vs. Household Debt\");\n",
    "\n",
    "Nice! You can see all four of our clusters, each differentiated from the rest by color.\n",
    "\n",
    "We're going to make one more visualization, converting the cluster analysis we just did to something a little more actionable: a side-by-side bar chart. In order to do that, we need to put our clustered data into a DataFrame.\n",
    "\n",
    "VimeoVideo(\"713918023\", h=\"110156bd98\", width=600)\n",
    "Task 6.2.19: Create a DataFrame xgb that contains the mean \"DEBT\" and \"HOUSES\" values for each of the clusters in your final_model.\n",
    "\n",
    "Access an object in a pipeline in scikit-learn.\n",
    "Aggregate data using the groupby method in pandas.\n",
    "Create a DataFrame from a Series in pandas.\n",
    "xgb = X.groupby(final_model.labels_).mean()\n",
    "​\n",
    "print(\"xgb type:\", type(xgb))\n",
    "print(\"xgb shape:\", xgb.shape)\n",
    "xgb\n",
    "xgb type: <class 'pandas.core.frame.DataFrame'>\n",
    "xgb shape: (4, 2)\n",
    "DEBT\tHOUSES\n",
    "0\t8.488629e+04\t1.031872e+05\n",
    "1\t1.838410e+07\t3.448400e+07\n",
    "2\t2.420929e+06\t4.551429e+06\n",
    "3\t5.472800e+06\t1.407400e+07\n",
    "final_model.cluster_centers_\n",
    "array([[   84886.28951384,   103187.22476563],\n",
    "       [18384100.        , 34484000.        ],\n",
    "       [ 2420928.57142857,  4551428.57142857],\n",
    "       [ 5472800.        , 14074000.        ]])\n",
    "Before you move to the next task, print out the cluster_centers_ for your final_model. Do you see any similarities between them and the DataFrame you just made? Why do you think that is?\n",
    "\n",
    "VimeoVideo(\"713917740\", h=\"bcc496c2d9\", width=600)\n",
    "Task 6.2.20: Create a side-by-side bar chart from xgb that shows the mean \"DEBT\" and \"HOUSES\" values for each of the clusters in your final_model. For readability, you'll want to divide the values in xgb by 1 million. Be sure to label the x-axis \"Cluster\", the y-axis \"Value [$1 million]\", and use the title \"Mean Home Value & Household Debt by Cluster\".\n",
    "\n",
    "Create a bar chart using pandas.\n",
    "# Create side-by-side bar chart of `xgb`\n",
    "(xgb/1e6).plot(kind=\"bar\")\n",
    "plt.xlabel(\"Cluster\")\n",
    "plt.ylabel(\"Value [$1 million]\")\n",
    "plt.title(\"Mean Home Value & Household Debt by Cluster\");\n",
    "\n",
    "#to get the ratio of debt to houses\n",
    "(xgb[\"DEBT\"]/xgb[\"HOUSES\"]).plot(\n",
    "    kind=\"bar\",\n",
    "    x=\"Cluster\",\n",
    "    y=\"Proportion: DEBT/HOME\",\n",
    "    title=\"Proportion of Debt to Home value\"\n",
    ");\n",
    "\n",
    "In this plot, we have our four clusters spread across the x-axis, and the dollar amounts for home value and household debt on the y-axis.\n",
    "\n",
    "The first thing to look at in this chart is the different mean home values for the five clusters. Clusters 0 represents households with small to moderate home values, clusters 2 and 3 have high home values, and cluster 1 has extremely high values.\n",
    "\n",
    "The second thing to look at is the proportion of debt to home value. In clusters 1 and 3, this proportion is around 0.5. This suggests that these groups have a moderate amount of untapped equity in their homes. But for group 0, it's almost 1, which suggests that the largest source of household debt is their mortgage. Group 2 is unique in that they have the smallest proportion of debt to home value, around 0.4.\n",
    "\n",
    "This information could be useful to financial institution that want to target customers with products that would appeal to them. For instance, households in group 0 might be interested in refinancing their mortgage to lower their interest rate. Group 2 households could be interested in a home equity line of credit because they have more equity in their homes. And the bankers, Bill Gates, and Beyoncés in group 1 might want white-glove personalized wealth management.\n",
    "\n",
    "Copyright 2023 WorldQuant University. This content is licensed solely for personal use. Redistribution or publication of this material is strictly prohibited.\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
