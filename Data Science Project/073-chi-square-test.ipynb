{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "7.3. Chi-square test\n",
    "\n",
    "In the previous lesson, we identified a subset of applicants who don't complete the admissions quiz. Then we developed a null and alternative hypothesis that we want to test in an experiment.\n",
    "\n",
    "In this lesson, we'll conduct our experiment. First, we'll determine how long we need to run our experiment in order to detect a significant difference between our control and treatment groups. Then we'll run our experiment and evaluate our results using a chi-square test.\n",
    "\n",
    "Warning: The database has changed since this videos for this lesson were filmed. So don't worry if you don't get exactly the same numbers as the instructor for the tasks in this project.\n",
    "import math\n",
    "​\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import scipy\n",
    "import wqet_grader\n",
    "from IPython.display import VimeoVideo\n",
    "from pymongo import MongoClient\n",
    "from statsmodels.stats.contingency_tables import Table2x2\n",
    "from statsmodels.stats.power import GofChisquarePower\n",
    "from teaching_tools.ab_test.experiment import Experiment\n",
    "from teaching_tools.ab_test.reset import Reset\n",
    "​\n",
    "wqet_grader.init(\"Project 7 Assessment\")\n",
    "​\n",
    "​\n",
    "# Reset database\n",
    "r = Reset()\n",
    "r.reset_database()\n",
    "Reset 'ds-applicants' collection. Now has 5025 documents.\n",
    "Reset 'mscfe-applicants' collection. Now has 1335 documents.\n",
    "VimeoVideo(\"742459144\", h=\"0f1aa2db83\", width=600)\n",
    "Preparing the Experiment\n",
    "Connect to Database\n",
    "Just like in the previous module, the first thing we need to do is connect to our MongoDB server and turn our collection of interest into a variable.\n",
    "\n",
    "Task 7.3.1: Assign the \"ds-applicants\" collection in the \"wqu-abtest\" database to the variable name ds_app.\n",
    "\n",
    "What's a MongoDB collection?\n",
    "Access a collection in a database using PyMongo.\n",
    "client = MongoClient(host=\"localhost\", port=27017)\n",
    "db=client[\"wqu-abtest\"]\n",
    "ds_app = db[\"ds-applicants\"]\n",
    "print(\"client:\", type(client))\n",
    "print(\"ds_app:\", type(ds_app))\n",
    "client: <class 'pymongo.mongo_client.MongoClient'>\n",
    "ds_app: <class 'pymongo.collection.Collection'>\n",
    "Calculate Power\n",
    "One of a Data Scientist's jobs is to help others determine what's meaningful information and what's not. You can think about this as distinguishing between signal and noise. As the author Nate Silver puts it, \"The signal is the truth. The noise is what distracts us from the truth.\"\n",
    "\n",
    "In our experiment, we're looking for a signal indicating that applicants who receive an email are more likely to complete the admissions quiz. If signal's strong, it'll be easy to see. A much higher number of applicants in our treatment group will complete the quiz. But if the signal's weak and there's only a tiny change in quiz completion, it will be harder to determine if this is a meaningful difference or just random variation. How can we separate signal from noise in this case? The answer is statistical power.\n",
    "\n",
    "To understand what statistical power is, let's imagine that we're radio engineers building an antenna. The size of our antenna would depend on the type of signal we wanted to detect. It would be OK to build a low-power antenna if we only wanted to detect strong signals, like a car antenna that picks up your favorite local music station. But our antenna wouldn't pick up weaker signals — like a radio station on the other side of the globe. For weaker signals, we'd need something with higher power. In statistics, power comes from the number of observations you include in your experiment. In other words, the more people we include, the stronger our antenna, and the better we can detect weak signals.\n",
    "\n",
    "To determine exactly how many people we should include in our study, we need to do a power calculation.\n",
    "\n",
    "VimeoVideo(\"734517993\", h=\"624e1cd2ea\", width=600)\n",
    "VimeoVideo(\"734517709\", h=\"907b2d3102\", width=600)\n",
    "Task 7.3.2: First, instantiate a GofChisquarePower object and assign it to the variable name chi_square_power. Then use it to calculate the group_size needed to detect an effect size of 0.2, with an alpha of 0.05 and power of 0.8.\n",
    "\n",
    "What's statistical power?\n",
    "What's effect size?\n",
    "Perform a power calculation using statsmodels.\n",
    "chi_square_power = GofChisquarePower()\n",
    "group_size = math.ceil(\n",
    "    chi_square_power.solve_power(effect_size = 0.2, alpha=0.05, power=0.8)\n",
    "    )\n",
    "​\n",
    "print(\"Group size:\", group_size)\n",
    "print(\"Total # of applicants needed:\", group_size * 2)\n",
    "Group size: 197\n",
    "Total # of applicants needed: 394\n",
    "The results here are telling us that if we want to detect an effect size of 0.2 we need a group size of about 200 people. Since our experiment has two conditions (treatment and control, or email and no email), that means we need a total of about 400 applicants in our experiment.\n",
    "\n",
    "But what about detecting other effect sizes? If we needed to detect a larger effect size, we'd need fewer applicants. If we needed to detect a smaller effect size, we'd need more applicants. One way to visualize the relationship between effect size, statistical power, and number of applicants is to make a graph.\n",
    "\n",
    "VimeoVideo(\"734517244\", h=\"44460ba891\", width=600)\n",
    "Task 7.3.3: Use chi_square_power to plot a power curve for three effect sizes: 0.2, 0.5, and 0.8. The x-axis should be the number of observations, ranging from 0 to twice the group_size from the previous task.\n",
    "\n",
    "Plot a power calculation using statsmodels.\n",
    "n_observations = np.arange(0, group_size * 2 +1)\n",
    "effect_sizes = np.array([0.2, 0.5, 0.8])\n",
    "​\n",
    "# Plot power curve using `chi_square_power`\n",
    "chi_square_power.plot_power(\n",
    "    dep_var=\"nobs\",\n",
    "    nobs=n_observations,\n",
    "    effect_size=effect_sizes,\n",
    "    alpha=0.05,\n",
    "    n_bins=2\n",
    ")\n",
    "\n",
    "\n",
    "Calculate Subjects per Day\n",
    "In the previous lesson, we decided that our experiment would focus on the subset of applicants who don't take the admissions quiz immediately after creating an account. We know we need around 400 observations from this subset, but how long do we need to run our experiment for in order to get that number?\n",
    "\n",
    "To answer that question, we first need to calculate how many such applicants open an account each day.\n",
    "\n",
    "VimeoVideo(\"734516984\", h=\"f8c2ae9e0e\", width=600)\n",
    "Task 7.3.4: Use the aggregate method to calculate how many new accounts were created each day included in the database.\n",
    "\n",
    "Perform aggregation calculations on documents using PyMongo.\n",
    "Use the $dateTrunc operator to truncate a date in PyMongo.\n",
    "Use the $group operator in an aggregation in PyMongo.\n",
    "Use the $match operator in an aggregation in PyMongo.\n",
    "result = ds_app.aggregate(\n",
    "    [\n",
    "        {\"$match\": {\"admissionsQuiz\": \"incomplete\"}},\n",
    "        {\n",
    "            \"$group\": {\n",
    "                \"_id\": {\"$dateTrunc\": {\"date\": \"$createdAt\", \"unit\": \"day\"}},\n",
    "                \"count\": {\"$sum\": 1}\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "​\n",
    "print(\"result type:\", type(result))\n",
    "result type: <class 'pymongo.command_cursor.CommandCursor'>\n",
    "Now we'll read our query result into a Series.\n",
    "\n",
    "VimeoVideo(\"734516829\", h=\"9c7014eb8d\", width=600)\n",
    "Task 7.3.5: Read your result from the previous task into the Series no_quiz. The Series index should be called \"date\", and the name should be \"new_users\".\n",
    "\n",
    "Create a DataFrame from a dictionary using pandas.\n",
    "Rename columns in a DataFrame using pandas.\n",
    "Set and reset the index of a DataFrame in pandas.\n",
    "Sort a DataFrame or Series in pandas.\n",
    "no_quiz = (\n",
    "    pd.DataFrame(result)\n",
    "    .rename({\"_id\": \"date\", \"count\": \"new_users\"}, axis=1)\n",
    "    .set_index(\"date\")\n",
    "    .sort_index()\n",
    "    .squeeze()\n",
    ")\n",
    "​\n",
    "print(\"no_quiz type:\", type(no_quiz))\n",
    "print(\"no_quiz shape:\", no_quiz.shape)\n",
    "no_quiz.head()\n",
    "no_quiz type: <class 'pandas.core.series.Series'>\n",
    "no_quiz shape: (30,)\n",
    "date\n",
    "2022-05-01    37\n",
    "2022-05-02    49\n",
    "2022-05-03    43\n",
    "2022-05-04    48\n",
    "2022-05-05    47\n",
    "Name: new_users, dtype: int64\n",
    "Okay! Let's see what we've got here by creating a histogram.\n",
    "\n",
    "VimeoVideo(\"734516524\", h=\"c1e506e702\", width=600)\n",
    "Task 7.3.6: Create a histogram of no_quiz. Be sure to label the x-axis \"New Users with No Quiz\", the y-axis \"Frequency [count]\", and use the title \"Distribution of Daily New Users with No Quiz\".\n",
    "\n",
    "Create a histogram using pandas.\n",
    "# Create histogram of `no_quiz`\n",
    "no_quiz.hist()\n",
    "# Add axis labels and title\n",
    "plt.xlabel(\"No_Quiz Applicants\"),\n",
    "plt.ylabel(\"Frequency [count]\")\n",
    "plt.title(\"Distribution of Daily New Users with No Quiz\")\n",
    "Text(0.5, 1.0, 'Distribution of Daily New Users with No Quiz')\n",
    "\n",
    "We can see that somewhere between 30–60 no-quiz applicants come to the site every day. But how can we use this information to ensure that we get our 400 observations? We need to calculate the mean and standard deviation of this distribution.\n",
    "\n",
    "VimeoVideo(\"734516130\", h=\"a93fabac0f\", width=600)\n",
    "Task 7.3.7: Calculate the mean and standard deviation of the values in no_quiz, and assign them to the variables mean and std, respectively.\n",
    "\n",
    "Calculate summary statistics for a DataFrame or Series in pandas.\n",
    "mean = no_quiz.describe()[\"mean\"]\n",
    "std = no_quiz.describe()[\"std\"]\n",
    "print(\"no_quiz mean:\", mean)\n",
    "print(\"no_quiz std:\", std)\n",
    "no_quiz mean: 43.6\n",
    "no_quiz std: 6.398275629767974\n",
    "The exact answers you'll get here will be a little different, but you should see a mean around 40 and a standard deviation between 7 and 8. Taking those rough numbers as a guide, how many days do we need to run the experiment to make sure we get to 400 users?\n",
    "\n",
    "Intuitively, you might think the answer is 10 days, because  10⋅40=400\n",
    " . But we can't guarantee that we'll get 40 new users every day. Some days, there will be fewer; some days, more. So how can we estimate how many days we'll need? Statistics!\n",
    "\n",
    "The distribution we plotted above shows how many no-quiz applicants come to the site each day, but we can use that mean and standard deviation to create a new distribution — one for the sum of no-quiz applicants over several days. Let's start with our intuition, and create a distribution for 10 days.\n",
    "\n",
    "VimeoVideo(\"742459088\", h=\"1962b016f9\", width=600)\n",
    "Task 7.3.8: Calculate the mean and standard deviation of the probability distribution for the total number of sign-ups over 10 days.\n",
    "\n",
    "What's the central limit theorem?\n",
    "days = 10\n",
    "sum_mean = mean * days\n",
    "sum_std = std * np.sqrt(days)\n",
    "print(\"Mean of sum:\", sum_mean)\n",
    "print(\"Std of sum:\", sum_std)\n",
    "Mean of sum: 436.0\n",
    "Std of sum: 20.233124087615032\n",
    "With this new distribution, we want to know what the probability is that we'll have 400 or more no-quiz applicants after 10 days. We can calculate this using the cumulative density function or CDF. The CDF will give us the probability of having 400 or fewer no-quiz applicants, so we'll need to subtract our result from 1.\n",
    "\n",
    "VimeoVideo(\"742459015\", h=\"33ad7b37ca\", width=600)\n",
    "Task 7.3.9: Calculate the probability of getting 400 or more sign-ups over three days.\n",
    "\n",
    "What's a cumulative density function?\n",
    "Calculate the cumulative density function for a normal distribution using SciPy.\n",
    "prob_400_or_fewer = scipy.stats.norm.cdf(\n",
    "    group_size * 2,\n",
    "    loc=sum_mean,\n",
    "    scale=sum_std\n",
    ")\n",
    "prob_400_or_greater = 1 - prob_400_or_fewer\n",
    "​\n",
    "print(\n",
    "    f\"Probability of getting 400+ no_quiz in {days} days:\",\n",
    "    round(prob_400_or_greater, 3),\n",
    ")\n",
    "Probability of getting 400+ no_quiz in 10 days: 0.981\n",
    "Again, the exact probability will change every time we regenerate the database, but there should be around a 90% chance that we'll get the number of applicants we need over 10 days.\n",
    "\n",
    "Since we're talking about finding an optimal timeframe, though, try out some other possibilities. Try changing the value of days in Task 7.3.8, and see what happens when you run 7.3.9. Cool, huh?\n",
    "\n",
    "Running the Experiment\n",
    "Okay, now we know how many applicants we need and what the timeframe needs to be. Let's actually run the experiment!\n",
    "\n",
    "VimeoVideo(\"734515713\", h=\"7702f5163d\", width=600)\n",
    "Task 7.3.10: Using the Experiment object created below, run your experiment for the appropriate number of days.\n",
    "\n",
    "#Instantiate Experiment \n",
    "exp = Experiment(repo=client, db=\"wqu-abtest\", collection=\"ds-applicants\")\n",
    "exp.reset_experiment()\n",
    "​\n",
    "#Run Experiment\n",
    "result = exp.run_experiment(days=days)\n",
    "print(\"result type:\", type(result))\n",
    "result\n",
    "result type: <class 'dict'>\n",
    "{'acknowledged': True, 'inserted_count': 1666}\n",
    "Evaluating Experiment Results\n",
    "After all that work, the actual running of the experiment might seem a little anticlimactic. This is because we automated the process and are working with synthetic data. Let's look at our results?\n",
    "\n",
    "Get Data\n",
    "First, get the data we need by finding just the people who were part of the experiment...\n",
    "\n",
    "VimeoVideo(\"734515601\", h=\"759340caf1\", width=600)\n",
    "Task 7.3.11: Query ds_app to find all the documents that are part of the experiment.\n",
    "\n",
    "Query a collection using PyMongo.\n",
    "result = ds_app.find({\"inExperiment\": True})\n",
    "print(\"results type:\", type(result))\n",
    "results type: <class 'pymongo.cursor.Cursor'>\n",
    "...and load them into a DataFrame.\n",
    "\n",
    "VimeoVideo(\"734515308\", h=\"8308ce4a22\", width=600)\n",
    "Task 7.3.12: Load your result from the previous task into the DataFrame df. Be sure to drop any rows with NaN values.\n",
    "\n",
    "Create a DataFrame from a dictionary using pandas.\n",
    "Drop rows with missing values from a DataFrame using pandas.\n",
    "df = pd.DataFrame(result).dropna()\n",
    "​\n",
    "print(\"df type:\", type(df))\n",
    "print(\"df shape:\", df.shape)\n",
    "df.head()\n",
    "df type: <class 'pandas.core.frame.DataFrame'>\n",
    "df shape: (448, 12)\n",
    "_id\tcreatedAt\tfirstName\tlastName\temail\tbirthday\tgender\thighestDegreeEarned\tcountryISO2\tadmissionsQuiz\tinExperiment\tgroup\n",
    "0\t679364712fa8eb8937f3b517\t2025-01-24 20:51:54\tRobert\tAngell\trobert.angell1@gmall.com\t1998-08-30\tmale\tBachelor's degree\tMX\tcomplete\tTrue\temail (t)\n",
    "1\t679364712fa8eb8937f3b51b\t2025-01-30 22:03:27\tJim\tMccarthy\tjim.mccarthy80@microsift.com\t1979-12-05\tmale\tBachelor's degree\tPK\tcomplete\tTrue\temail (t)\n",
    "2\t679364712fa8eb8937f3b51d\t2025-01-28 17:23:37\tLaura\tDelaney\tlaura.delaney25@gmall.com\t1996-01-18\tfemale\tBachelor's degree\tKE\tcomplete\tTrue\temail (t)\n",
    "3\t679364712fa8eb8937f3b51e\t2025-01-31 22:05:22\tBetty\tSkaggs\tbetty.skaggs52@hotmeal.com\t1993-01-13\tfemale\tSome College (1-3 years)\tEG\tcomplete\tTrue\temail (t)\n",
    "4\t679364712fa8eb8937f3b523\t2025-01-27 07:58:52\tVincent\tMathis\tvincent.mathis45@gmall.com\t2005-07-10\tmale\tBachelor's degree\tPK\tcomplete\tTrue\temail (t)\n",
    "Build Contingency Table\n",
    "Now that the results are in a DataFrame, we can start pulling apart what we found. Let's start by making a table showing how many people did and didn't complete the quiz across our two groups.\n",
    "\n",
    "VimeoVideo(\"734514187\", h=\"9063c1eccf\", width=600)\n",
    "Task 7.3.13: Use pandas crosstab to create a 2x2 table data that shows how many applicants in each experimental group completed and didn't complete the admissions quiz. After you're done, submit your data to the grader.\n",
    "\n",
    "What's cross tabulation?\n",
    "Compute a cross tabulation in pandas.\n",
    "data = pd.crosstab(\n",
    "    index=df[\"group\"],\n",
    "    columns=df[\"admissionsQuiz\"],\n",
    "    normalize=False #changing to True gives you in percentages\n",
    ")\n",
    "​\n",
    "print(\"data type:\", type(data))\n",
    "print(\"data shape:\", data.shape)\n",
    "data\n",
    "data type: <class 'pandas.core.frame.DataFrame'>\n",
    "data shape: (2, 2)\n",
    "admissionsQuiz\tcomplete\tincomplete\n",
    "group\t\t\n",
    "email (t)\t17\t207\n",
    "no email (c)\t12\t212\n",
    "wqet_grader.grade(\"Project 7 Assessment\", \"Task 7.3.13\", data)\n",
    "​\n",
    "Yes! Great problem solving.\n",
    "\n",
    "Score: 1\n",
    "\n",
    "Just to make it easier to see, let's show the results in a side-by-side bar chart.\n",
    "\n",
    "VimeoVideo(\"734513651\", h=\"cc012589ac\", width=600)\n",
    "Task 7.3.14: Create a function that returns side-by-side bar chart from data, showing the number of complete and incomplete quizzes for both the treatment and control groups. Be sure to label the x-axis \"Group\", the y-axis \"Frequency [count]\", and use the title \"Admissions Quiz Completion by Group\".\n",
    "\n",
    "What's a bar chart?\n",
    "Create a bar chart using plotly express.WQU WorldQuant University Applied Data Science Lab QQQQ\n",
    "def build_contingency_bar():\n",
    "    # Create side-by-side bar chart\n",
    "    fig = px.bar(\n",
    "        data_frame=data,\n",
    "        barmode=\"group\",\n",
    "        title=\"Admissions Quiz Completion by Group\"\n",
    "    )\n",
    "    # Set axis labels\n",
    "    fig.update_layout(\n",
    "        xaxis_title=\"Group\", \n",
    "        yaxis_title=\"Frequency [count]\",\n",
    "        legend={\"title\": \"Admissions Quiz\"}\n",
    "        \n",
    "    )\n",
    "    return fig\n",
    "​\n",
    "​\n",
    "build_contingency_bar().show()\n",
    "\n",
    "Without doing anything else, we can see that people who got an email actually did complete the quiz more often than people who didn't. So can we conclude that, as a general rule, applicants who receive an email are more likely to complete quiz. No, not yet. After all, the difference we see could be due to chance.\n",
    "\n",
    "In order to determine if this difference is more than random variation, we need to take our results, put them into a contingency table, and run a statistical test.\n",
    "\n",
    "VimeoVideo(\"734512752\", h=\"92e79c3f89\", width=600)\n",
    "Task 7.3.15: Instantiate a Table2x2 object named contingency_table, using the values from the data you created in the previous task.\n",
    "\n",
    "What's a contingency table?\n",
    "Create a contingency table using statsmodels.\n",
    "contingency_table = Table2x2(data.values)\n",
    "​\n",
    "print(\"contingency_table type:\", type(contingency_table))\n",
    "contingency_table.table_orig\n",
    "contingency_table type: <class 'statsmodels.stats.contingency_tables.Table2x2'>\n",
    "array([[ 17, 207],\n",
    "       [ 12, 212]])\n",
    "#make sure the treatment group is ontop in you contigency table \n",
    "#and the yes column is first \n",
    "#if not you will get a wrong output\n",
    "Now that we have our table, we can calculate what we would expect to see if there was no difference quiz completion between our two groups.\n",
    "\n",
    "VimeoVideo(\"734512565\", h=\"4e29a856e1\", width=600)\n",
    "Task 7.3.16: Calculate the fitted values for your contigency_table.\n",
    "\n",
    "Calculate the fitted values for a contingency table in statsmodels.\n",
    "# Calculate fitted values\n",
    "contingency_table.fittedvalues\n",
    "array([[ 14.5, 209.5],\n",
    "       [ 14.5, 209.5]])\n",
    "These are the counts, but what about probabilities?\n",
    "\n",
    "VimeoVideo(\"734512366\", h=\"70d4db3edd\", width=600)\n",
    "Task 7.3.17: Calculate the joint probabilities under independence for your contingency_table.\n",
    "\n",
    "Calculate the joint probabilities for a contingency table in statsmodels.\n",
    "# Calculate independent joint probabilities\n",
    "contingency_table.independence_probabilities.round(3)\n",
    "array([[0.032, 0.468],\n",
    "       [0.032, 0.468]])\n",
    "Conduct Chi-Square Test\n",
    "Here's where the rubber meets the road: all the previous calculations have shown us that some of the people who got an email went on to complete the quiz, but we don't know what might be driving that effect. After all, some people might be responding to getting an email, but others might have finished the quiz whether we emailed them or not. Either way, the effect we found could just as easily be due to chance as it could be a result of something we did. The only way to find out whether the result is due to chance is to calculate statistical significance.\n",
    "\n",
    "There are several ways to do this, but since the rows and columns here are unordered (nominal factors), we can do a chi-square test.\n",
    "\n",
    "VimeoVideo(\"742458959\", h=\"e8da1aeecf\", width=600)\n",
    "Task 7.3.18: Perform a chi-square test of independence on your contingency_table and assign the results to chi_square_test.\n",
    "\n",
    "What's a chi-square test of independence?\n",
    "Perform a chi-square test on a contingency table in statsmodels.\n",
    "chi_square_test = contingency_table.test_nominal_association()\n",
    "​\n",
    "print(\"chi_square_test type:\", type(chi_square_test))\n",
    "print(chi_square_test)\n",
    "chi_square_test type: <class 'statsmodels.stats.contingency_tables._Bunch'>\n",
    "df          1\n",
    "pvalue      0.33701987275401357\n",
    "statistic   0.9217348366389596\n",
    "The important part of that result is the p-value. We set our threshold for significance at 0.05 way back at the beginning, so, for our results to be statistically significant, the p-value needs to be less than or equal to 0.05. Our p-value is much higher than 0.05, which means that the difference we saw in our side-by-side bar graph is probably due to chance. In other words, it's noise, not signal. So we can't reject our null hypothesis.\n",
    "\n",
    "What does this result mean? It means there may not be any difference between the groups, or that the difference is so small that we don't have the statistical power to detect it.\n",
    "\n",
    "Since this is a simulated experiment, we can actually increase the power by re-running the experiment for a longer time. If we ran the experiment for 60 days, we might end up with a statistically-significant result. Try it and see what happens!\n",
    "\n",
    "However, there are two important things to keep in mind. First, just because a result is statistically significant doesn't mean that it's practically significant. A 1% increase in quiz completion may not be worth the time or resources needed to run an email campaign every day. Second, when the number of observations gets very large, any small difference is going to appear statistically significant. This increases the risk of a false positive — rejecting our null hypothesis when it's actually true.\n",
    "\n",
    "Setting the issue of significance aside for now, there's one more calculation that can be helpful in sharing the results of an experiment: the odds ratio. In other words, how much more likely is someone in the treatment group to complete the quiz versus someone in the control group?\n",
    "\n",
    "VimeoVideo(\"734512125\", h=\"8dbc500ec2\", width=600)\n",
    "Task 7.3.19: Calculate the odds ratio for your contingency_table.\n",
    "\n",
    "What's an odds ratio in a chi-square test?\n",
    "Calculate the odds ratio from a chi-square test in statsmodels.\n",
    "odds_ratio = contingency_table.oddsratio.round(1)\n",
    "print(\"Odds ratio:\", odds_ratio)\n",
    "Odds ratio: 1.5\n",
    "The interpretation here is that for every 1 person who doesn't complete the quiz, about 1.3 people do. Keep in mind, though, that this ratio isn't actionable in the case of our experiment because our results weren't statistically significant.\n",
    "\n",
    "The last thing we need to do is print all the values in our contingency table.\n",
    "\n",
    "VimeoVideo(\"748065153\", h=\"47f74a0df8\", width=600)\n",
    "Task 7.3.20: Print out the summary for your contingency_table.\n",
    "\n",
    "What's a contingency table?\n",
    "Create a contingency table using statsmodels.\n",
    "summary = contingency_table.summary()\n",
    "print(\"summary type:\", type(summary))\n",
    "summary\n",
    "summary type: <class 'statsmodels.iolib.table.SimpleTable'>\n",
    "Estimate\tSE\tLCB\tUCB\tp-value\n",
    "Odds ratio\t1.451\t\t0.676\t3.113\t0.339\n",
    "Log odds ratio\t0.372\t0.389\t-0.391\t1.136\t0.339\n",
    "Risk ratio\t1.417\t\t0.693\t2.897\t0.340\n",
    "Log risk ratio\t0.348\t0.365\t-0.367\t1.064\t0.340\n",
    "Copyright 2023 WorldQuant University. This content is licensed solely for personal use. Redistribution or publication of this material is strictly prohibited.\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
