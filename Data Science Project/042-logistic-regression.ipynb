{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "4.2. Predicting Damage with Logistic Regression\n",
    "\n",
    "import sqlite3\n",
    "import warnings\n",
    "â€‹\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from category_encoders import OneHotEncoder\n",
    "from IPython.display import VimeoVideo\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.utils.validation import check_is_fitted\n",
    "â€‹\n",
    "warnings.simplefilter(action=\"ignore\", category=FutureWarning)\n",
    "VimeoVideo(\"665414074\", h=\"d441543f18\", width=600)\n",
    "Prepare Data\n",
    "Import\n",
    "def wrangle(db_path):\n",
    "    # Connect to database\n",
    "    conn = sqlite3.connect(db_path)\n",
    "â€‹\n",
    "    # Construct query\n",
    "    query = \"\"\"\n",
    "        SELECT distinct(i.building_id) AS b_id,\n",
    "           s.*,\n",
    "           d.damage_grade\n",
    "        FROM id_map AS i\n",
    "        JOIN building_structure AS s ON i.building_id = s.building_id\n",
    "        JOIN building_damage AS d ON i.building_id = d.building_id\n",
    "        WHERE district_id = 4\n",
    "    \"\"\"\n",
    "â€‹\n",
    "    # Read query results into DataFrame\n",
    "    df = pd.read_sql(query, conn, index_col=\"b_id\")\n",
    "    \n",
    "    #Identify Leaky columns\n",
    "    drop_cols = [col for col in df.columns if \"post_eq\" in col]    \n",
    "    \n",
    "    #Create binary target\n",
    "    \n",
    "    df[\"damage_grade\"] = df[\"damage_grade\"].str[-1].astype(int)\n",
    "    df[\"severe_damage\"] = (df[\"damage_grade\"] > 3).astype(int)\n",
    "    \n",
    "    #Drop old target\n",
    "    \n",
    "    drop_cols.append(\"damage_grade\")\n",
    "    \n",
    "    #Drop multicolinearity column\n",
    "    drop_cols.append(\"count_floors_pre_eq\")\n",
    "    \n",
    "    #Drop high cardinality \n",
    "    drop_cols.append(\"building_id\")\n",
    "    \n",
    "    #Drop columns\n",
    "    df.drop(columns = drop_cols, inplace=True)\n",
    "    \n",
    "    return df\n",
    "VimeoVideo(\"665414541\", h=\"dfe22afdfb\", width=600)\n",
    "Task 4.2.1: Complete the wrangle function above so that the it returns the results of query as a DataFrame. Be sure that the index column is set to \"b_id\". Also, the path to the SQLite database is \"/home/jovyan/nepal.sqlite\".\n",
    "\n",
    "Read SQL query into a DataFrame using pandas.\n",
    "Write a function in Python.\n",
    "df = wrangle(\"/home/jovyan/nepal.sqlite\")\n",
    "df.head()\n",
    "age_building\tplinth_area_sq_ft\theight_ft_pre_eq\tland_surface_condition\tfoundation_type\troof_type\tground_floor_type\tother_floor_type\tposition\tplan_configuration\tsuperstructure\tsevere_damage\n",
    "b_id\t\t\t\t\t\t\t\t\t\t\t\t\n",
    "164002\t20\t560\t18\tFlat\tMud mortar-Stone/Brick\tBamboo/Timber-Light roof\tMud\tTImber/Bamboo-Mud\tNot attached\tRectangular\tStone, mud mortar\t0\n",
    "164081\t21\t200\t12\tFlat\tMud mortar-Stone/Brick\tBamboo/Timber-Light roof\tMud\tTImber/Bamboo-Mud\tNot attached\tRectangular\tStone, mud mortar\t0\n",
    "164089\t18\t315\t20\tFlat\tMud mortar-Stone/Brick\tBamboo/Timber-Light roof\tMud\tTImber/Bamboo-Mud\tNot attached\tRectangular\tStone, mud mortar\t0\n",
    "164098\t45\t290\t13\tFlat\tMud mortar-Stone/Brick\tBamboo/Timber-Light roof\tMud\tTImber/Bamboo-Mud\tNot attached\tRectangular\tStone, mud mortar\t0\n",
    "164103\t21\t230\t13\tFlat\tMud mortar-Stone/Brick\tBamboo/Timber-Light roof\tMud\tTImber/Bamboo-Mud\tNot attached\tRectangular\tStone, mud mortar\t0\n",
    "# Check your work\n",
    "assert df.shape[0] == 70836, f\"`df` should have 70,836 rows, not {df.shape[0]}.\"\n",
    "There seem to be several features in df with information about the condition of a property after the earthquake.\n",
    "\n",
    "VimeoVideo(\"665414560\", h=\"ad4bba19ed\", width=600)\n",
    "Task 4.2.2: Add to your wrangle function so that these features are dropped from the DataFrame. Don't forget to rerun all the cells above.\n",
    "\n",
    "Drop a column from a DataFrame using pandas.\n",
    "Subset a DataFrame's columns based on column names in pandas.\n",
    "#drop_cols = []\n",
    "#for cols in df.columns:\n",
    " #   if \"post_eq\" in cols:\n",
    "  #      drop_col.append(cols):\n",
    "            \n",
    "#drop_cols = [col for col in df.columns if \"post_eq\" in col]         \n",
    "            \n",
    "#drop_cols\n",
    "print(df.info())\n",
    "<class 'pandas.core.frame.DataFrame'>\n",
    "Int64Index: 70836 entries, 164002 to 234835\n",
    "Data columns (total 12 columns):\n",
    " #   Column                  Non-Null Count  Dtype \n",
    "---  ------                  --------------  ----- \n",
    " 0   age_building            70836 non-null  int64 \n",
    " 1   plinth_area_sq_ft       70836 non-null  int64 \n",
    " 2   height_ft_pre_eq        70836 non-null  int64 \n",
    " 3   land_surface_condition  70836 non-null  object\n",
    " 4   foundation_type         70836 non-null  object\n",
    " 5   roof_type               70836 non-null  object\n",
    " 6   ground_floor_type       70836 non-null  object\n",
    " 7   other_floor_type        70836 non-null  object\n",
    " 8   position                70836 non-null  object\n",
    " 9   plan_configuration      70836 non-null  object\n",
    " 10  superstructure          70836 non-null  object\n",
    " 11  severe_damage           70836 non-null  int64 \n",
    "dtypes: int64(4), object(8)\n",
    "memory usage: 7.0+ MB\n",
    "None\n",
    "# Check your work\n",
    "assert (\n",
    "    df.filter(regex=\"post_eq\").shape[1] == 0\n",
    "), \"`df` still has leaky features. Try again!\"\n",
    "We want to build a binary classification model, but our current target \"damage_grade\" has more than two categories.\n",
    "\n",
    "VimeoVideo(\"665414603\", h=\"12b3d2f23e\", width=600)\n",
    "Task 4.2.3: Add to your wrangle function so that it creates a new target column \"severe_damage\". For buildings where the \"damage_grade\" is Grade 4 or above, \"severe_damage\" should be 1. For all other buildings, \"severe_damage\" should be 0. Don't forget to drop \"damage_grade\" to avoid leakage, and rerun all the cells above.\n",
    "\n",
    "Access a substring in a Series using pandas.\n",
    "Drop a column from a DataFrame using pandas.\n",
    "Recast a column as a different data type in pandas.\n",
    "#df[\"damage_grade\"] = df[\"damage_grade\"].str[-1].astype(int)\n",
    "#df[\"severe_damage\"] = (df[\"damage_grade\"] > 3).astype(int)\n",
    "print(df[\"severe_damage\"].value_counts())\n",
    "1    45519\n",
    "0    25317\n",
    "Name: severe_damage, dtype: int64\n",
    "# Check your work\n",
    "assert (\n",
    "    \"damage_grade\" not in df.columns\n",
    "), \"Your DataFrame should not include the `'damage_grade'` column.\"\n",
    "assert (\n",
    "    \"severe_damage\" in df.columns\n",
    "), \"Your DataFrame is missing the `'severe_damage'` column.\"\n",
    "assert (\n",
    "    df[\"severe_damage\"].value_counts().shape[0] == 2\n",
    "), f\"The `'damage_grade'` column should have only two unique values, not {df['severe_damage'].value_counts().shape[0]}\"\n",
    "Explore\n",
    "Since our model will be a type of linear model, we need to make sure there's no issue with multicollinearity in our dataset.\n",
    "\n",
    "VimeoVideo(\"665414636\", h=\"d34256b4e3\", width=600)\n",
    "Task 4.2.4: Plot a correlation heatmap of the remaining numerical features in df. Since \"severe_damage\" will be your target, you don't need to include it in your heatmap.\n",
    "\n",
    "What's a correlation coefficient?\n",
    "What's a heatmap?\n",
    "Create a correlation matrix in pandas.\n",
    "Create a heatmap in seaborn.\n",
    "Do you see any features that you need to drop?\n",
    "\n",
    "â€‹\n",
    "# Create correlation matrix\n",
    "correlation = df.select_dtypes(\"number\").drop(columns = \"severe_damage\").corr()\n",
    "# Plot heatmap of `correlation`\n",
    "sns.heatmap(correlation)\n",
    "<Axes: >\n",
    "\n",
    "Task 4.2.5: Change wrangle function so that it drops the \"count_floors_pre_eq\" column. Don't forget to rerun all the cells above.\n",
    "\n",
    "Drop a column from a DataFrame using pandas.\n",
    "# Check your work\n",
    "assert (\n",
    "    \"count_floors_pre_eq\" not in df.columns\n",
    "), \"Did you drop the `'count_floors_pre_eq'` column?\"\n",
    "Before we build our model, let's see if we can identify any obvious differences between houses that were severely damaged in the earthquake (\"severe_damage\"==1) those that were not (\"severe_damage\"==0). Let's start with a numerical feature.\n",
    "\n",
    "VimeoVideo(\"665414667\", h=\"f39c2c21bc\", width=600)\n",
    "Task 4.2.6: Use seaborn to create a boxplot that shows the distributions of the \"height_ft_pre_eq\" column for both groups in the \"severe_damage\" column. Remember to label your axes.\n",
    "\n",
    "What's a boxplot?\n",
    "Create a boxplot using Matplotlib.\n",
    "#box plot can be used to visualize a data between a contious variable and a categorical variable\n",
    "# you can create a box plot using pandas or matplotlib, seaborn\n",
    "# Create boxplot\n",
    "sns.boxplot(x=\"severe_damage\", y=\"height_ft_pre_eq\", data=df)\n",
    "#seaborn is built ontop of matplotlib so we can use the matplotlib tools we have\n",
    "â€‹\n",
    "# Label axes\n",
    "plt.xlabel(\"Severe Damage\")\n",
    "plt.ylabel(\"height Pre Equake[ft]\")\n",
    "plt.title(\"Distribution of Building Class by Height\");\n",
    "\n",
    "Before we move on to the many categorical features in this dataset, it's a good idea to see the balance between our two classes. What percentage were severely damaged, what percentage were not?\n",
    "\n",
    "VimeoVideo(\"665414684\", h=\"81295d5bdb\", width=600)\n",
    "Task 4.2.7: Create a bar chart of the value counts for the \"severe_damage\" column. You want to calculate the relative frequencies of the classes, not the raw count, so be sure to set the normalize argument to True.\n",
    "\n",
    "What's a bar chart?\n",
    "What's a majority class?\n",
    "What's a minority class?\n",
    "Aggregate data in a Series using value_counts in pandas.\n",
    "Create a bar chart using pandas.\n",
    "# Plot value counts of `\"severe_damage\"`\n",
    "df[\"severe_damage\"].value_counts(normalize=True).plot(\n",
    "    kind=\"bar\", xlabel=\"Class\", ylabel=\"Relative Frequency\", title=\"Class Balance\");\n",
    "â€‹\n",
    "\n",
    "VimeoVideo(\"665414697\", h=\"ee2d4f28c6\", width=600)\n",
    "Task 4.2.8: Create two variables, majority_class_prop and minority_class_prop, to store the normalized value counts for the two classes in df[\"severe_damage\"].\n",
    "\n",
    "Aggregate data in a Series using value_counts in pandas.\n",
    "majority_class_prop, minority_class_prop = df[\"severe_damage\"].value_counts(normalize=True)\n",
    "â€‹\n",
    "print(majority_class_prop, minority_class_prop)\n",
    "0.6425969845841097 0.3574030154158902\n",
    "# Check your work\n",
    "assert (\n",
    "    majority_class_prop < 1\n",
    "), \"`majority_class_prop` should be a floating point number between 0 and 1.\"\n",
    "assert (\n",
    "    minority_class_prop < 1\n",
    "), \"`minority_class_prop` should be a floating point number between 0 and 1.\"\n",
    "VimeoVideo(\"665414718\", h=\"6a1e0c1e53\", width=600)\n",
    "Task 4.2.9: Are buildings with certain foundation types more likely to suffer severe damage? Create a pivot table of df where the index is \"foundation_type\" and the values come from the \"severe_damage\" column, aggregated by the mean.\n",
    "\n",
    "What's a pivot table?\n",
    "Reshape a DataFrame based on column values in pandas.\n",
    "# Create pivot table\n",
    "foundation_pivot = pd.pivot_table(\n",
    "    df, index=\"foundation_type\", values=\"severe_damage\", aggfunc=np.mean\n",
    ").sort_values(by=\"severe_damage\")\n",
    "foundation_pivot\n",
    "severe_damage\n",
    "foundation_type\t\n",
    "RC\t0.026224\n",
    "Bamboo/Timber\t0.324074\n",
    "Cement-Stone/Brick\t0.421908\n",
    "Mud mortar-Stone/Brick\t0.687792\n",
    "Other\t0.818898\n",
    "VimeoVideo(\"665414734\", h=\"46de83f96e\", width=600)\n",
    "Task 4.2.10: How do the proportions in foundation_pivot compare to the proportions for our majority and minority classes? Plot foundation_pivot as horizontal bar chart, adding vertical lines at the values for majority_class_prop and minority_class_prop.\n",
    "\n",
    "What's a bar chart?\n",
    "Add a vertical or horizontal line across a plot using Matplotlib.\n",
    "Create a bar chart using pandas.\n",
    "# Plot bar chart of `foundation_pivot`\n",
    "foundation_pivot.plot(kind=\"barh\")\n",
    "plt.axvline(majority_class_prop, linestyle=\"--\", color=\"red\", label=\"Majority Class\")\n",
    "plt.axvline(minority_class_prop, linestyle=\"--\", color=\"green\", label=\"minority class\")\n",
    "â€‹\n",
    "plt.legend(loc=\"lower right\");\n",
    "â€‹\n",
    "â€‹\n",
    "â€‹\n",
    "\n",
    "VimeoVideo(\"665414748\", h=\"8549a0f89c\", width=600)\n",
    "Task 4.2.11: Combine the select_dtypes and nunique methods to see if there are any high- or low-cardinality categorical features in the dataset.\n",
    "\n",
    "What are high- and low-cardinality features?\n",
    "Determine the unique values in a column using pandas.\n",
    "Subset a DataFrame's columns based on the column data types in pandas.\n",
    "# Check for high- and low-cardinality categorical features\n",
    "df.select_dtypes(\"object\").nunique()\n",
    "land_surface_condition     3\n",
    "foundation_type            5\n",
    "roof_type                  3\n",
    "ground_floor_type          5\n",
    "other_floor_type           4\n",
    "position                   4\n",
    "plan_configuration        10\n",
    "superstructure            11\n",
    "dtype: int64\n",
    "Split\n",
    "Task 4.2.12: Create your feature matrix X and target vector y. Your target is \"severe_damage\".\n",
    "\n",
    "What's a feature matrix?\n",
    "What's a target vector?\n",
    "Subset a DataFrame by selecting one or more columns in pandas.\n",
    "Select a Series from a DataFrame in pandas.\n",
    "target = \"severe_damage\"\n",
    "X = df.drop(columns=target)\n",
    "y = df[target]\n",
    "VimeoVideo(\"665414769\", h=\"1bfddf07b2\", width=600)\n",
    "Task 4.2.13: Divide your data (X and y) into training and test sets using a randomized train-test split. Your test set should be 20% of your total data. And don't forget to set a random_state for reproducibility.\n",
    "\n",
    "Perform a randomized train-test split using scikit-learn.\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "â€‹\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"y_test shape:\", y_test.shape)\n",
    "X_train shape: (56668, 11)\n",
    "y_train shape: (56668,)\n",
    "X_test shape: (14168, 11)\n",
    "y_test shape: (14168,)\n",
    "Frequent Question: Why do we set the random state to 42?\n",
    "\n",
    "Answer: The truth is you can pick any integer when setting a random state. The number you choose doesn't affect the results of your project; it just makes sure that your work is reproducible so that others can verify it. However, lots of people choose 42 because it appears in a well-known work of science fiction called The Hitchhiker's Guide to the Galaxy. In short, it's an inside joke. ðŸ˜‰\n",
    "\n",
    "Build Model\n",
    "Baseline\n",
    "VimeoVideo(\"665414807\", h=\"c997c58720\", width=600)\n",
    "Task 4.2.14: Calculate the baseline accuracy score for your model.\n",
    "\n",
    "What's accuracy score?\n",
    "Aggregate data in a Series using value_counts in pandas.WQU WorldQuant University Applied Data Science Lab QQQQ\n",
    "y_train.value_counts(normalize=True).max()\n",
    "0.6410319757182183\n",
    "#accuracy score is used in classification not mae. acc score goes from 0-1\n",
    "acc_baseline = y_train.value_counts(normalize=True).max()\n",
    "print(\"Baseline Accuracy:\", round(acc_baseline, 2))\n",
    "Baseline Accuracy: 0.64\n",
    "Iterate\n",
    "VimeoVideo(\"665414835\", h=\"1d8673223e\", width=600)\n",
    "Task 4.2.15: Create a pipeline named model that contains a OneHotEncoder transformer and a LogisticRegression predictor. Be sure you set the use_cat_names argument for your transformer to True. Then fit it to the training data.\n",
    "\n",
    "What's logistic regression?\n",
    "What's one-hot encoding?\n",
    "Create a pipeline in scikit-learn.\n",
    "Fit a model to training data in scikit-learn.\n",
    "Tip: If you get a ConvergenceWarning when you fit your model to the training data, don't worry. This can sometimes happen with logistic regression models. Try setting the max_iter argument in your predictor to 1000.\n",
    "# Build model\n",
    "model = make_pipeline(\n",
    "    OneHotEncoder(use_cat_names=True),\n",
    "    LogisticRegression()\n",
    ")\n",
    "# Fit model to training data\n",
    "model.fit(X_train,y_train)\n",
    "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
    "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
    "\n",
    "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
    "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
    "Please also refer to the documentation for alternative solver options:\n",
    "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
    "  n_iter_i = _check_optimize_result(\n",
    "Pipeline(steps=[('onehotencoder',\n",
    "                 OneHotEncoder(cols=['land_surface_condition',\n",
    "                                     'foundation_type', 'roof_type',\n",
    "                                     'ground_floor_type', 'other_floor_type',\n",
    "                                     'position', 'plan_configuration',\n",
    "                                     'superstructure'],\n",
    "                               use_cat_names=True)),\n",
    "                ('logisticregression', LogisticRegression())])\n",
    "In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook.\n",
    "On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\n",
    "# Check your work\n",
    "assert isinstance(\n",
    "    model, Pipeline\n",
    "), f\"`model` should be a Pipeline, not type {type(model)}.\"\n",
    "assert isinstance(\n",
    "    model[0], OneHotEncoder\n",
    "), f\"The first step in your Pipeline should be a OneHotEncoder, not type {type(model[0])}.\"\n",
    "assert isinstance(\n",
    "    model[-1], LogisticRegression\n",
    "), f\"The last step in your Pipeline should be LogisticRegression, not type {type(model[-1])}.\"\n",
    "check_is_fitted(model)\n",
    "Evaluate\n",
    "VimeoVideo(\"665414885\", h=\"f35ff0e23e\", width=600)\n",
    "Task 4.2.16: Calculate the training and test accuracy scores for your models.\n",
    "\n",
    "Calculate the accuracy score for a model in scikit-learn.\n",
    "Generate predictions using a trained model in scikit-learn.\n",
    "#one way to calc accuracy score\n",
    "#accuracy_score(y_train, model.predict(X_train))\n",
    "#second way is \n",
    "model.score(X_test, y_test)\n",
    "0.7183794466403162\n",
    "acc_train = accuracy_score(y_train, model.predict(X_train))\n",
    "acc_test = model.score(X_test, y_test)\n",
    "â€‹\n",
    "print(\"Training Accuracy:\", round(acc_train, 2))\n",
    "print(\"Test Accuracy:\", round(acc_test, 2))\n",
    "Training Accuracy: 0.71\n",
    "Test Accuracy: 0.72\n",
    "Communicate\n",
    "VimeoVideo(\"665414902\", h=\"f9bdbe9e75\", width=600)\n",
    "Task 4.2.17: Instead of using the predict method with your model, try predict_proba with your training data. How does the predict_proba output differ than that of predict? What does it represent?\n",
    "\n",
    "Generate probability estimates using a trained model in scikit-learn.\n",
    "model.predict(X_train)[:5]\n",
    "array([0, 0, 1, 1, 1])\n",
    "y_train_pred_proba = model.predict_proba(X_train)\n",
    "print(y_train_pred_proba[:5])\n",
    "[[0.97960398 0.02039602]\n",
    " [0.5585624  0.4414376 ]\n",
    " [0.31103037 0.68896963]\n",
    " [0.46124096 0.53875904]\n",
    " [0.31562805 0.68437195]]\n",
    "Task 4.2.18: Extract the feature names and importances from your model.\n",
    "\n",
    "Access an object in a pipeline in scikit-learn.\n",
    "features = model.named_steps[\"onehotencoder\"].get_feature_names()\n",
    "importances = model.named_steps[\"logisticregression\"].coef_[0]\n",
    "VimeoVideo(\"665414916\", h=\"c0540604cd\", width=600)\n",
    "Task 4.2.19: Create a pandas Series named odds_ratios, where the index is features and the values are your the exponential of the importances. How does odds_ratios for this model look different from the other linear models we made in projects 2 and 3?\n",
    "\n",
    "Create a Series in pandas.\n",
    "feat_imp = pd.Series(np.exp(importances), index=features).sort_values()\n",
    "feat_imp.head()\n",
    "ground_floor_type_RC                   0.372767\n",
    "roof_type_RCC/RB/RBC                   0.432499\n",
    "foundation_type_RC                     0.449994\n",
    "other_floor_type_RCC/RB/RBC            0.546477\n",
    "superstructure_Brick, cement mortar    0.579689\n",
    "dtype: float64\n",
    "odds_ratios = feat_imp\n",
    "odds_ratios.head()\n",
    "ground_floor_type_RC                   0.372767\n",
    "roof_type_RCC/RB/RBC                   0.432499\n",
    "foundation_type_RC                     0.449994\n",
    "other_floor_type_RCC/RB/RBC            0.546477\n",
    "superstructure_Brick, cement mortar    0.579689\n",
    "dtype: float64\n",
    "VimeoVideo(\"665414943\", h=\"56eb74d93e\", width=600)\n",
    "Task 4.2.20: Create a horizontal bar chart with the five largest coefficients from odds_ratios. Be sure to label your x-axis \"Odds Ratio\".\n",
    "\n",
    "What's a bar chart?\n",
    "Create a bar chart using Matplotlib.\n",
    "# Horizontal bar chart, five largest coefficients\n",
    "odds_ratios.tail().plot(kind=\"barh\")\n",
    "plt.xlabel(\"Odds Ratio\")\n",
    "â€‹\n",
    "Text(0.5, 0, 'Odds Ratio')\n",
    "\n",
    "VimeoVideo(\"665414964\", h=\"a61b881450\", width=600)\n",
    "Task 4.2.21: Create a horizontal bar chart with the five smallest coefficients from odds_ratios. Be sure to label your x-axis \"Odds Ratio\".\n",
    "\n",
    "What's a bar chart?\n",
    "Create a bar chart using Matplotlib.\n",
    "\n",
    "\n",
    "# Horizontal bar chart, five smallest coefficients\n",
    "odds_ratios.head().plot(kind=\"barh\")\n",
    "plt.xlabel(\"Odds Ratio\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
