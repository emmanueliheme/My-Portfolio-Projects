{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "4.5. Earthquake Damage in Kavrepalanchok üá≥üáµ\n",
    "\n",
    "In this assignment, you'll build a classification model to predict building damage for the district of Kavrepalanchok.\n",
    "\n",
    "import warnings\n",
    "‚Äã\n",
    "import wqet_grader\n",
    "‚Äã\n",
    "warnings.simplefilter(action=\"ignore\", category=FutureWarning)\n",
    "wqet_grader.init(\"Project 4 Assessment\")\n",
    "# Import libraries here\n",
    "import sqlite3\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from category_encoders import OneHotEncoder\n",
    "from category_encoders import OrdinalEncoder\n",
    "from IPython.display import VimeoVideo\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.utils.validation import check_is_fitted\n",
    "‚Äã\n",
    "‚Äã\n",
    "Prepare Data\n",
    "Connect\n",
    "Run the cell below to connect to the nepal.sqlite database.WQU WorldQuant University Applied Data Science Lab QQQQ\n",
    "\n",
    "%load_ext sql\n",
    "%sql sqlite:////home/jovyan/nepal.sqlite\n",
    "There's a new jupysql version available (0.10.16), you're running 0.10.1. To upgrade: pip install jupysql --upgrade\n",
    "Warning:Be careful with your SQL queries in this assignment. If you try to get all the rows from a table (for example, SELECT * FROM id_map), you will cause an Out of Memory error on your virtual machine. So always include a LIMIT when first exploring a database.\n",
    "Task 4.5.1: What districts are represented in the id_map table? Determine the unique values in the district_id column.\n",
    "\n",
    "%%sql\n",
    "‚Äã\n",
    "SELECT DISTINCT(district_id)\n",
    "FROM id_map\n",
    "‚Äã\n",
    "Running query in 'sqlite:////home/jovyan/nepal.sqlite'\n",
    "district_id\n",
    "1\n",
    "2\n",
    "3\n",
    "4\n",
    "result = _.DataFrame().squeeze()  # noqa F821\n",
    "‚Äã\n",
    "wqet_grader.grade(\"Project 4 Assessment\", \"Task 4.5.1\", result)\n",
    "Awesome work.\n",
    "\n",
    "Score: 1\n",
    "\n",
    "What's the district ID for Kavrepalanchok? From the lessons, you already know that Gorkha is 4; from the textbook, you know that Ramechhap is 2. Of the remaining districts, Kavrepalanchok is the one with the largest number of observations in the id_map table.\n",
    "\n",
    "Task 4.5.2: Calculate the number of observations in the id_map table associated with district 1.\n",
    "\n",
    "%%sql\n",
    "SELECT COUNT (district_id)\n",
    "FROM id_map\n",
    "WHERE district_id = 1\n",
    "‚Äã\n",
    "Running query in 'sqlite:////home/jovyan/nepal.sqlite'\n",
    "COUNT (district_id)\n",
    "36112\n",
    "result = [_.DataFrame().astype(float).squeeze()]  # noqa F821\n",
    "wqet_grader.grade(\"Project 4 Assessment\", \"Task 4.5.2\", result)\n",
    "Yes! Your hard work is paying off.\n",
    "\n",
    "Score: 1\n",
    "\n",
    "Task 4.5.3: Calculate the number of observations in the id_map table associated with district 3.\n",
    "\n",
    "%%sql\n",
    "SELECT COUNT (district_id)\n",
    "FROM id_map\n",
    "WHERE district_id = 3\n",
    "‚Äã\n",
    "‚Äã\n",
    "Running query in 'sqlite:////home/jovyan/nepal.sqlite'\n",
    "COUNT (district_id)\n",
    "82684\n",
    "result = [_.DataFrame().astype(float).squeeze()]  # noqa F821\n",
    "wqet_grader.grade(\"Project 4 Assessment\", \"Task 4.5.3\", result)\n",
    "You got it. Dance party time! üï∫üíÉüï∫üíÉ\n",
    "\n",
    "Score: 1\n",
    "\n",
    "Task 4.5.4: Join the unique building IDs from Kavrepalanchok in id_map, all the columns from building_structure, and the damage_grade column from building_damage, limiting your results to 5 rows. Make sure you rename the building_id column in id_map as b_id and limit your results to the first five rows of the new table.\n",
    "\n",
    "%%sql\n",
    "SELECT DISTINCT(i.building_id) AS b_id,\n",
    "    s.*,\n",
    "    d.damage_grade\n",
    "FROM id_map AS i\n",
    "JOIN building_structure AS s ON i.building_id = s.building_id\n",
    "JOIN building_damage AS d ON i.building_id = d.building_id\n",
    "WHERE district_id = 3\n",
    "‚Äã\n",
    "LIMIT 5\n",
    "Running query in 'sqlite:////home/jovyan/nepal.sqlite'\n",
    "b_id\tbuilding_id\tcount_floors_pre_eq\tcount_floors_post_eq\tage_building\tplinth_area_sq_ft\theight_ft_pre_eq\theight_ft_post_eq\tland_surface_condition\tfoundation_type\troof_type\tground_floor_type\tother_floor_type\tposition\tplan_configuration\tcondition_post_eq\tsuperstructure\tdamage_grade\n",
    "87473\t87473\t2\t1\t15\t382\t18\t7\tFlat\tMud mortar-Stone/Brick\tBamboo/Timber-Light roof\tMud\tTImber/Bamboo-Mud\tNot attached\tRectangular\tDamaged-Used in risk\tStone, mud mortar\tGrade 4\n",
    "87479\t87479\t1\t0\t12\t328\t7\t0\tFlat\tMud mortar-Stone/Brick\tBamboo/Timber-Light roof\tMud\tNot applicable\tNot attached\tRectangular\tDamaged-Rubble clear\tStone, mud mortar\tGrade 5\n",
    "87482\t87482\t2\t1\t23\t427\t20\t7\tFlat\tMud mortar-Stone/Brick\tBamboo/Timber-Light roof\tMud\tTImber/Bamboo-Mud\tNot attached\tRectangular\tDamaged-Not used\tStone, mud mortar\tGrade 4\n",
    "87491\t87491\t2\t1\t12\t427\t14\t7\tFlat\tMud mortar-Stone/Brick\tBamboo/Timber-Light roof\tMud\tTImber/Bamboo-Mud\tNot attached\tRectangular\tDamaged-Not used\tStone, mud mortar\tGrade 4\n",
    "87496\t87496\t2\t0\t32\t360\t18\t0\tFlat\tMud mortar-Stone/Brick\tBamboo/Timber-Light roof\tMud\tTImber/Bamboo-Mud\tNot attached\tRectangular\tDamaged-Rubble clear\tStone, mud mortar\tGrade 5\n",
    "result = _.DataFrame().set_index(\"b_id\")  # noqa F821\n",
    "‚Äã\n",
    "wqet_grader.grade(\"Project 4 Assessment\", \"Task 4.5.4\", result)\n",
    "Awesome work.\n",
    "\n",
    "Score: 1\n",
    "\n",
    "Import\n",
    "Task 4.5.5: Write a wrangle function that will use the query you created in the previous task to create a DataFrame. In addition your function should:\n",
    "\n",
    "Create a \"severe_damage\" column, where all buildings with a damage grade greater than 3 should be encoded as 1. All other buildings should be encoded at 0.\n",
    "Drop any columns that could cause issues with leakage or multicollinearity in your model.\n",
    "# Build your `wrangle` function here\n",
    "def wrangle(db_path):\n",
    "    # Connect to database\n",
    "    conn = sqlite3.connect(db_path)\n",
    "‚Äã\n",
    "    # Construct query\n",
    "    query = \"\"\"\n",
    "    SELECT DISTINCT(i.building_id) AS b_id,\n",
    "        s.*,\n",
    "        d.damage_grade\n",
    "    FROM id_map AS i\n",
    "    JOIN building_structure AS s ON i.building_id = s.building_id\n",
    "    JOIN building_damage AS d ON i.building_id = d.building_id\n",
    "    WHERE district_id = 3\n",
    "    \"\"\"\n",
    "‚Äã\n",
    "    # Read query results into DataFrame\n",
    "    df = pd.read_sql(query, conn, index_col=\"b_id\")\n",
    "    \n",
    "    #Identify Leaky columns\n",
    "    drop_cols = [col for col in df.columns if \"post_eq\" in col]    \n",
    "    \n",
    "    #Create binary target\n",
    "    \n",
    "    df[\"damage_grade\"] = df[\"damage_grade\"].str[-1].astype(int)\n",
    "    df[\"severe_damage\"] = (df[\"damage_grade\"] > 3).astype(int)\n",
    "    \n",
    "    #Drop old target\n",
    "    \n",
    "    drop_cols.append(\"damage_grade\")\n",
    "    \n",
    "    #Drop multicolinearity column\n",
    "    drop_cols.append(\"count_floors_pre_eq\")\n",
    "    \n",
    "    #Drop high cardinality \n",
    "    drop_cols.append(\"building_id\")\n",
    "    \n",
    "    #Drop columns\n",
    "    df.drop(columns = drop_cols, inplace=True)\n",
    "    \n",
    "    \n",
    "    return df\n",
    "Use your wrangle function to query the database at \"/home/jovyan/nepal.sqlite\" and return your cleaned results.\n",
    "\n",
    "df = wrangle(\"/home/jovyan/nepal.sqlite\")\n",
    "df.head()\n",
    "age_building\tplinth_area_sq_ft\theight_ft_pre_eq\tland_surface_condition\tfoundation_type\troof_type\tground_floor_type\tother_floor_type\tposition\tplan_configuration\tsuperstructure\tsevere_damage\n",
    "b_id\t\t\t\t\t\t\t\t\t\t\t\t\n",
    "87473\t15\t382\t18\tFlat\tMud mortar-Stone/Brick\tBamboo/Timber-Light roof\tMud\tTImber/Bamboo-Mud\tNot attached\tRectangular\tStone, mud mortar\t1\n",
    "87479\t12\t328\t7\tFlat\tMud mortar-Stone/Brick\tBamboo/Timber-Light roof\tMud\tNot applicable\tNot attached\tRectangular\tStone, mud mortar\t1\n",
    "87482\t23\t427\t20\tFlat\tMud mortar-Stone/Brick\tBamboo/Timber-Light roof\tMud\tTImber/Bamboo-Mud\tNot attached\tRectangular\tStone, mud mortar\t1\n",
    "87491\t12\t427\t14\tFlat\tMud mortar-Stone/Brick\tBamboo/Timber-Light roof\tMud\tTImber/Bamboo-Mud\tNot attached\tRectangular\tStone, mud mortar\t1\n",
    "87496\t32\t360\t18\tFlat\tMud mortar-Stone/Brick\tBamboo/Timber-Light roof\tMud\tTImber/Bamboo-Mud\tNot attached\tRectangular\tStone, mud mortar\t1\n",
    "print(df.info())\n",
    "<class 'pandas.core.frame.DataFrame'>\n",
    "Int64Index: 76533 entries, 87473 to 163998\n",
    "Data columns (total 12 columns):\n",
    " #   Column                  Non-Null Count  Dtype \n",
    "---  ------                  --------------  ----- \n",
    " 0   age_building            76533 non-null  int64 \n",
    " 1   plinth_area_sq_ft       76533 non-null  int64 \n",
    " 2   height_ft_pre_eq        76533 non-null  int64 \n",
    " 3   land_surface_condition  76533 non-null  object\n",
    " 4   foundation_type         76533 non-null  object\n",
    " 5   roof_type               76533 non-null  object\n",
    " 6   ground_floor_type       76533 non-null  object\n",
    " 7   other_floor_type        76533 non-null  object\n",
    " 8   position                76533 non-null  object\n",
    " 9   plan_configuration      76533 non-null  object\n",
    " 10  superstructure          76533 non-null  object\n",
    " 11  severe_damage           76533 non-null  int64 \n",
    "dtypes: int64(4), object(8)\n",
    "memory usage: 7.6+ MB\n",
    "None\n",
    "‚Äã\n",
    "wqet_grader.grade(\n",
    "    \"Project 4 Assessment\", \"Task 4.5.5\", wrangle(\"/home/jovyan/nepal.sqlite\")\n",
    ")\n",
    "ü•≥\n",
    "\n",
    "Score: 1\n",
    "\n",
    "Explore\n",
    "Task 4.5.6: Are the classes in this dataset balanced? Create a bar chart with the normalized value counts from the \"severe_damage\" column. Be sure to label the x-axis \"Severe Damage\" and the y-axis \"Relative Frequency\". Use the title \"Kavrepalanchok, Class Balance\".\n",
    "\n",
    "# Plot value counts of `\"severe_damage\"`\n",
    "df[\"severe_damage\"].value_counts(normalize=True).plot(\n",
    "    kind=\"bar\", xlabel=\"Class\", ylabel=\"Relative Frequency\", title=\"Class Balance\");\n",
    "‚Äã\n",
    "# Don't delete the code below üëá\n",
    "plt.savefig(\"images/4-5-6.png\", dpi=150)\n",
    "‚Äã\n",
    "\n",
    "with open(\"images/4-5-6.png\", \"rb\") as file:\n",
    "    wqet_grader.grade(\"Project 4 Assessment\", \"Task 4.5.6\", file)\n",
    "Good work!\n",
    "\n",
    "Score: 1\n",
    "\n",
    "Task 4.5.7: Is there a relationship between the footprint size of a building and the damage it sustained in the earthquake? Use seaborn to create a boxplot that shows the distributions of the \"plinth_area_sq_ft\" column for both groups in the \"severe_damage\" column. Label your x-axis \"Severe Damage\" and y-axis \"Plinth Area [sq. ft.]\". Use the title \"Kavrepalanchok, Plinth Area vs Building Damage\".\n",
    "\n",
    "sns.boxplot(x=\"severe_damage\", y=\"plinth_area_sq_ft\", data=df)\n",
    "#seaborn is built ontop of matplotlib so we can use the matplotlib tools we have\n",
    "‚Äã\n",
    "# Label axes\n",
    "plt.xlabel(\"Severe Damage\")\n",
    "plt.ylabel(\"Plinth Area [sq.ft]\")\n",
    "plt.title(\"Kavrepalanchok, Plinth Area vs Building Damage\");\n",
    "‚Äã\n",
    "# Don't delete the code below üëá\n",
    "plt.savefig(\"images/4-5-7.png\", dpi=150)\n",
    "‚Äã\n",
    "\n",
    "with open(\"images/4-5-7.png\", \"rb\") as file:\n",
    "    wqet_grader.grade(\"Project 4 Assessment\", \"Task 4.5.7\", file)\n",
    "You = coding ü•∑\n",
    "\n",
    "Score: 1\n",
    "\n",
    "Task 4.5.8: Are buildings with certain roof types more likely to suffer severe damage? Create a pivot table of df where the index is \"roof_type\" and the values come from the \"severe_damage\" column, aggregated by the mean.\n",
    "\n",
    "roof_pivot = pd.pivot_table(\n",
    "    df, index=\"roof_type\", values=\"severe_damage\", aggfunc=np.mean\n",
    ").sort_values(by=\"severe_damage\")\n",
    "roof_pivot\n",
    "severe_damage\n",
    "roof_type\t\n",
    "RCC/RB/RBC\t0.040715\n",
    "Bamboo/Timber-Heavy roof\t0.569477\n",
    "Bamboo/Timber-Light roof\t0.604842\n",
    "‚Äã\n",
    "wqet_grader.grade(\"Project 4 Assessment\", \"Task 4.5.8\", roof_pivot)\n",
    "Very impressive.\n",
    "\n",
    "Score: 1\n",
    "\n",
    "Split\n",
    "Task 4.5.9: Create your feature matrix X and target vector y. Your target is \"severe_damage\".\n",
    "\n",
    "target = \"severe_damage\"\n",
    "X = df.drop(columns = target)\n",
    "y = df[target]\n",
    "print(\"X shape:\", X.shape)\n",
    "print(\"y shape:\", y.shape)\n",
    "X shape: (76533, 11)\n",
    "y shape: (76533,)\n",
    "‚Äã\n",
    "wqet_grader.grade(\"Project 4 Assessment\", \"Task 4.5.9a\", X)\n",
    "Yup. You got it.\n",
    "\n",
    "Score: 1\n",
    "\n",
    "‚Äã\n",
    "wqet_grader.grade(\"Project 4 Assessment\", \"Task 4.5.9b\", y)\n",
    "Yes! Keep on rockin'. üé∏That's right.\n",
    "\n",
    "Score: 1\n",
    "\n",
    "Task 4.5.10: Divide your dataset into training and validation sets using a randomized split. Your validation set should be 20% of your data.\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, test_size = 0.2, random_state=42\n",
    ")\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"X_val shape:\", X_val.shape)\n",
    "print(\"y_val shape:\", y_val.shape)\n",
    "X_train shape: (61226, 11)\n",
    "y_train shape: (61226,)\n",
    "X_val shape: (15307, 11)\n",
    "y_val shape: (15307,)\n",
    "wqet_grader.grade(\"Project 4 Assessment\", \"Task 4.5.10\", [X_train.shape == (61226, 11)])\n",
    "Boom! You got it.\n",
    "\n",
    "Score: 1\n",
    "\n",
    "Build Model\n",
    "Baseline\n",
    "Task 4.5.11: Calculate the baseline accuracy score for your model.\n",
    "\n",
    "acc_baseline = y_train.value_counts(normalize=True).max()\n",
    "print(\"Baseline Accuracy:\", round(acc_baseline, 2))\n",
    "Baseline Accuracy: 0.55\n",
    "wqet_grader.grade(\"Project 4 Assessment\", \"Task 4.5.11\", [acc_baseline])\n",
    "Python master üòÅ\n",
    "\n",
    "Score: 1\n",
    "\n",
    "Iterate\n",
    "Task 4.5.12: Create a model model_lr that uses logistic regression to predict building damage. Be sure to include an appropriate encoder for categorical features.\n",
    "\n",
    "model_lr = make_pipeline(\n",
    "    OneHotEncoder(use_cat_names=True),\n",
    "    LogisticRegression()\n",
    ")\n",
    "# Fit model to training data\n",
    "model_lr.fit(X_train,y_train)\n",
    "‚Äã\n",
    "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
    "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
    "\n",
    "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
    "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
    "Please also refer to the documentation for alternative solver options:\n",
    "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
    "  n_iter_i = _check_optimize_result(\n",
    "Pipeline(steps=[('onehotencoder',\n",
    "                 OneHotEncoder(cols=['land_surface_condition',\n",
    "                                     'foundation_type', 'roof_type',\n",
    "                                     'ground_floor_type', 'other_floor_type',\n",
    "                                     'position', 'plan_configuration',\n",
    "                                     'superstructure'],\n",
    "                               use_cat_names=True)),\n",
    "                ('logisticregression', LogisticRegression())])\n",
    "In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook.\n",
    "On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\n",
    "‚Äã\n",
    "wqet_grader.grade(\"Project 4 Assessment\", \"Task 4.5.12\", model_lr)\n",
    "Awesome work.\n",
    "\n",
    "Score: 1\n",
    "\n",
    "Task 4.5.13: Calculate training and validation accuracy score for model_lr.\n",
    "\n",
    "lr_train_acc = accuracy_score(y_train, model_lr.predict(X_train))\n",
    "lr_val_acc = model_lr.score(X_val, y_val)\n",
    "‚Äã\n",
    "print(\"Logistic Regression, Training Accuracy Score:\", lr_train_acc)\n",
    "print(\"Logistic Regression, Validation Accuracy Score:\", lr_val_acc)\n",
    "Logistic Regression, Training Accuracy Score: 0.6511449384248522\n",
    "Logistic Regression, Validation Accuracy Score: 0.6519892859476056\n",
    "submission = [lr_train_acc, lr_val_acc]\n",
    "wqet_grader.grade(\"Project 4 Assessment\", \"Task 4.5.13\", submission)\n",
    "Wow, you're making great progress.\n",
    "\n",
    "Score: 1\n",
    "\n",
    "Task 4.5.14: Perhaps a decision tree model will perform better than logistic regression, but what's the best hyperparameter value for max_depth? Create a for loop to train and evaluate the model model_dt at all depths from 1 to 15. Be sure to use an appropriate encoder for your model, and to record its training and validation accuracy scores at every depth. The grader will evaluate your validation accuracy scores only.\n",
    "\n",
    "depth_hyperparams = range(1, 16)\n",
    "training_acc = []\n",
    "validation_acc = []\n",
    "for d in depth_hyperparams:\n",
    "    model_dt = make_pipeline(\n",
    "        OrdinalEncoder(), \n",
    "        DecisionTreeClassifier(max_depth = d, random_state = 42)\n",
    "    )\n",
    "    model_dt.fit(X_train, y_train)\n",
    "     # Calculate training accuracy score and append to `training_acc`\n",
    "    training_acc.append(model_dt.score(X_train, y_train))\n",
    "    # Calculate validation accuracy score and append to `training_acc`\n",
    "    validation_acc.append(model_dt.score(X_val, y_val))\n",
    "‚Äã\n",
    "print(\"Training Accuracy Scores:\", training_acc[:3])\n",
    "print(\"Validation Accuracy Scores:\", validation_acc[:3])\n",
    "    \n",
    "Training Accuracy Scores: [0.6303041191650606, 0.6303041191650606, 0.642292490118577]\n",
    "Validation Accuracy Scores: [0.6350035931273273, 0.6350035931273273, 0.6453909975828053]\n",
    "submission = pd.Series(validation_acc, index=depth_hyperparams)\n",
    "‚Äã\n",
    "wqet_grader.grade(\"Project 4 Assessment\", \"Task 4.5.14\", submission)\n",
    "Good work!\n",
    "\n",
    "Score: 1\n",
    "\n",
    "Task 4.5.15: Using the values in training_acc and validation_acc, plot the validation curve for model_dt. Label your x-axis \"Max Depth\" and your y-axis \"Accuracy Score\". Use the title \"Validation Curve, Decision Tree Model\", and include a legend.\n",
    "\n",
    "plt.plot(depth_hyperparams, training_acc, label=\"training\")\n",
    "plt.plot(depth_hyperparams, validation_acc, label=\"validation\")\n",
    "plt.xlabel(\"Max Depth\")\n",
    "plt.ylabel(\"Accuracy Score\")\n",
    "plt.legend();\n",
    "# Don't delete the code below üëá\n",
    "plt.savefig(\"images/4-5-15.png\", dpi=150)\n",
    "‚Äã\n",
    "\n",
    "with open(\"images/4-5-15.png\", \"rb\") as file:\n",
    "    wqet_grader.grade(\"Project 4 Assessment\", \"Task 4.5.15\", file)\n",
    "Yup. You got it.\n",
    "\n",
    "Score: 1\n",
    "\n",
    "Task 4.5.16: Build and train a new decision tree model final_model_dt, using the value for max_depth that yielded the best validation accuracy score in your plot above.\n",
    "\n",
    "final_model_dt = make_pipeline(\n",
    "    OrdinalEncoder(), \n",
    "    DecisionTreeClassifier(max_depth = submission.idxmax(), random_state = 42)\n",
    ")\n",
    "# Fit model to training data\n",
    "final_model_dt.fit(X_train,y_train)\n",
    "‚Äã\n",
    "‚Äã\n",
    "Pipeline(steps=[('ordinalencoder',\n",
    "                 OrdinalEncoder(cols=['land_surface_condition',\n",
    "                                      'foundation_type', 'roof_type',\n",
    "                                      'ground_floor_type', 'other_floor_type',\n",
    "                                      'position', 'plan_configuration',\n",
    "                                      'superstructure'],\n",
    "                                mapping=[{'col': 'land_surface_condition',\n",
    "                                          'data_type': dtype('O'),\n",
    "                                          'mapping': Flat              1\n",
    "Moderate slope    2\n",
    "Steep slope       3\n",
    "NaN              -2\n",
    "dtype: int64},\n",
    "                                         {'col': 'foundation_type',\n",
    "                                          'dat...\n",
    "Building with Central Courtyard     9\n",
    "H-shape                            10\n",
    "NaN                                -2\n",
    "dtype: int64},\n",
    "                                         {'col': 'superstructure',\n",
    "                                          'data_type': dtype('O'),\n",
    "                                          'mapping': Stone, mud mortar        1\n",
    "Adobe/mud                2\n",
    "Brick, cement mortar     3\n",
    "RC, engineered           4\n",
    "Brick, mud mortar        5\n",
    "Stone, cement mortar     6\n",
    "RC, non-engineered       7\n",
    "Timber                   8\n",
    "Other                    9\n",
    "Bamboo                  10\n",
    "Stone                   11\n",
    "NaN                     -2\n",
    "dtype: int64}])),\n",
    "                ('decisiontreeclassifier',\n",
    "                 DecisionTreeClassifier(max_depth=10, random_state=42))])\n",
    "In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook.\n",
    "On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\n",
    "‚Äã\n",
    "wqet_grader.grade(\"Project 4 Assessment\", \"Task 4.5.16\", final_model_dt)\n",
    "Party time! üéâüéâüéâ\n",
    "\n",
    "Score: 1\n",
    "\n",
    "Evaluate\n",
    "Task 4.5.17: How does your model perform on the test set? First, read the CSV file \"data/kavrepalanchok-test-features.csv\" into the DataFrame X_test. Next, use final_model_dt to generate a list of test predictions y_test_pred. Finally, submit your test predictions to the grader to see how your model performs.\n",
    "\n",
    "Tip: Make sure the order of the columns in X_test is the same as in your X_train. Otherwise, it could hurt your model's performance.\n",
    "\n",
    "X_test = pd.read_csv(\"data/kavrepalanchok-test-features.csv\", index_col=\"b_id\")\n",
    "y_test_pred = pd.Series(final_model_dt.predict(X_test))\n",
    "y_test_pred[:5]\n",
    "0    1\n",
    "1    1\n",
    "2    1\n",
    "3    1\n",
    "4    0\n",
    "dtype: int64\n",
    "submission = pd.Series(y_test_pred)\n",
    "wqet_grader.grade(\"Project 4 Assessment\", \"Task 4.5.17\", submission)\n",
    "Your model's accuracy score is 0.666. Wow, you're making great progress.\n",
    "\n",
    "Score: 1\n",
    "\n",
    "Communicate Results\n",
    "Task 4.5.18: What are the most important features for final_model_dt? Create a Series Gini feat_imp, where the index labels are the feature names for your dataset and the values are the feature importances for your model. Be sure that the Series is sorted from smallest to largest feature importance.\n",
    "\n",
    "features = X_train.columns\n",
    "importances = final_model_dt.named_steps[\"decisiontreeclassifier\"].feature_importances_\n",
    "‚Äã\n",
    "print(\"Features:\", features[:3])\n",
    "print(\"Importances:\", importances[:3])\n",
    "Features: Index(['age_building', 'plinth_area_sq_ft', 'height_ft_pre_eq'], dtype='object')\n",
    "Importances: [0.11264062 0.07312796 0.13072783]\n",
    "‚Äã\n",
    "feat_imp = pd.Series(importances, index=features).sort_values()\n",
    "feat_imp.head()\n",
    "plan_configuration        0.004189\n",
    "land_surface_condition    0.008599\n",
    "foundation_type           0.009967\n",
    "position                  0.011795\n",
    "ground_floor_type         0.013521\n",
    "dtype: float64\n",
    "‚Äã\n",
    "wqet_grader.grade(\"Project 4 Assessment\", \"Task 4.5.18\", feat_imp)\n",
    "You're making this look easy. üòâ\n",
    "\n",
    "Score: 1\n",
    "\n",
    "Task 4.5.19: Create a horizontal bar chart of feat_imp. Label your x-axis \"Gini Importance\" and your y-axis \"Feature\". Use the title \"Kavrepalanchok Decision Tree, Feature Importance\".\n",
    "\n",
    "Do you see any relationship between this plot and the exploratory data analysis you did regarding roof type?\n",
    "\n",
    "# Create horizontal bar chart of feature importances\n",
    "feat_imp.plot(kind=\"barh\")\n",
    "plt.xlabel(\"Gini Importance\")\n",
    "plt.ylabel(\"Feature\");\n",
    "# Don't delete the code below üëá\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"images/4-5-19.png\", dpi=150)\n",
    "‚Äã\n",
    "\n",
    "with open(\"images/4-5-19.png\", \"rb\") as file:\n",
    "    wqet_grader.grade(\"Project 4 Assessment\", \"Task 4.5.19\", file)\n",
    "Yup. You got it.\n",
    "\n",
    "Score: 1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Congratulations! You made it to the end of Project 4. üëèüëèüëè"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
