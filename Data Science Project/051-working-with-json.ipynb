{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "5.1. Working with JSON files\n",
    "\n",
    "In this project, we'll be looking at tracking corporate bankruptcies in Poland. To do that, we'll need to get data that's been stored in a JSON file, explore it, and turn it into a DataFrame that we'll use to train our model.\n",
    "\n",
    "import gzip\n",
    "import json\n",
    "â€‹\n",
    "import pandas as pd\n",
    "import wqet_grader\n",
    "from IPython.display import VimeoVideo\n",
    "â€‹\n",
    "wqet_grader.init(\"Project 5 Assessment\")\n",
    "â€‹\n",
    "VimeoVideo(\"694158732\", h=\"73c2fb4e4f\", width=600)\n",
    "Prepare Data\n",
    "Open\n",
    "The first thing we need to do is access the file that contains the data we need. We've done this using multiple strategies before, but this time around, we're going to use the command line.\n",
    "\n",
    "VimeoVideo(\"693794546\", h=\"6e1fab0a5e\", width=600)\n",
    "Task 5.1.1: Open a terminal window and navigate to the directory where the data for this project is located.\n",
    "\n",
    "What's the Linux command line?\n",
    "Navigate a file system using the Linux command line.\n",
    "As we've seen in our other projects, datasets can be large or small, messy or clean, and complex or easy to understand. Regardless of how the data looks, though, it needs to be saved in a file somewhere, and when that file gets too big, we need to compress it. Compressed files are easier to store because they take up less space. If you've ever come across a ZIP file, you've worked with compressed data.\n",
    "\n",
    "The file we're using for this project is compressed, so we'll need to use a file utility called gzip to open it up.\n",
    "\n",
    "VimeoVideo(\"693794604\", h=\"a8c0f15712\", width=600)\n",
    "Task 5.1.2: In the terminal window, locate the data file for this project and decompress it.\n",
    "\n",
    "What's gzip?\n",
    "What's data compression?\n",
    "Decompress a file using gzip.\n",
    "VimeoVideo(\"693794641\", h=\"d77bf46d41\", width=600)\n",
    "%%bash\n",
    "â€‹\n",
    "cd data\n",
    "â€‹\n",
    "gzip -dfk poland-bankruptcy-data-2009.json.gz\n",
    "Explore\n",
    "Now that we've decompressed the data, let's take a look and see what's there.\n",
    "\n",
    "VimeoVideo(\"693794658\", h=\"c8f1bba831\", width=600)\n",
    "Task 5.1.3: In the terminal window, examine the first 10 lines of poland-bankruptcy-data-2009.json.\n",
    "\n",
    "Print lines from a file in the Linux command line.\n",
    "Does this look like any of the data structures we've seen in previous projects?\n",
    "\n",
    "VimeoVideo(\"693794680\", h=\"7f1302444b\", width=600)\n",
    "Task 5.1.4: Open poland-bankruptcy-data-2009.json by opening the data folder to the left and then double-clicking on the file. ðŸ‘ˆ\n",
    "\n",
    "How is the data organized?\n",
    "\n",
    "Curly brackets? Key-value pairs? It looks similar to a Python dictionary. It's important to note that JSON is not exactly the same as a dictionary, but a lot of the same concepts apply. Let's try reading the file into a DataFrame and see what happens.\n",
    "\n",
    "VimeoVideo(\"693794696\", h=\"dd5b5ad116\", width=600)\n",
    "Task 5.1.5: Load the data into a DataFrame.\n",
    "\n",
    "Read a JSON file into a DataFrame using pandas.\n",
    "df = pd.read_json(\"data/poland-bankruptcy-data-2009.json\")\n",
    "df.head()\n",
    "VimeoVideo(\"693794711\", h=\"fdb009c4eb\", width=600)\n",
    "Hmmm. It looks like something went wrong, and we're going to have to fix it. Luckily for us, there's an error message to help us figure out what's happening here:\n",
    "\n",
    "ValueError: Mixing dicts with non-Series may lead to ambiguous ordering.\n",
    "\n",
    "What should we do? That error sounds serious, but the world is big, and we can't possibly be the first people to encounter this problem. When you come across an error, copy the message into a search engine and see what comes back. You'll get lots of results. The web has lots of places to look for solutions to problems like this one, and Stack Overflow is one of the best. Click here to check out a possible solution to our problem.\n",
    "\n",
    "There are three things to look for when you're browsing through solutions on Stack Overflow.\n",
    "\n",
    "Context: A good question is specific; if you click through that link, you'll see that the person asks a specific question, gives some relevant information about their OS and hardware, and then offers the code that threw the error. That's important, because we need...\n",
    "Reproducible Code: A good question also includes enough information for you to reproduce the problem yourself. After all, the only way to make sure the solution actually applies to your situation is to see if the code in the question throws the error you're having trouble with! In this case, the person included not only the code they used to get the error, but the actual error message itself. That would be useful on its own, but since you're looking for an actual solution to your problem, you're really looking for...\n",
    "An answer: Not every question on Stack Overflow gets answered. Luckily for us, the one we've been looking at did. There's a big green check mark next to the first solution, which means that the person who asked the question thought that solution was the best one.\n",
    "Let's try it and see if it works for us too!\n",
    "\n",
    "VimeoVideo(\"693794734\", h=\"fecea6a81e\", width=600)\n",
    "Task 5.1.6: Using a context manager, open the file poland-bankruptcy-data-2009.json and load it as a dictionary with the variable name poland_data.\n",
    "\n",
    "What's a context manager?\n",
    "Open a file in Python.\n",
    "Load a JSON file into a dictionary using Python.\n",
    "# Open file and load JSON\n",
    "with open(\"data/poland-bankruptcy-data-2009.json\", \"r\") as read_file:\n",
    "    poland_data = json.load(read_file)\n",
    "â€‹\n",
    "print(type(poland_data))\n",
    "<class 'dict'>\n",
    "Okay! Now that we've successfully opened up our dataset, let's take a look and see what's there, starting with the keys. Remember, the keys in a dictionary are categories of things in a dataset.WQU WorldQuant University Applied Data Science Lab QQQQ\n",
    "\n",
    "VimeoVideo(\"693794754\", h=\"18e70f4225\", width=600)\n",
    "Task 5.1.7: Print the keys for poland_data.\n",
    "\n",
    "List the keys of a dictionary in Python.\n",
    "# Print `poland_data` keys\n",
    "poland_data.keys()\n",
    "dict_keys(['schema', 'data', 'metadata'])\n",
    "schema tells us how the data is structured, metadata tells us where the data comes from, and data is the data itself.\n",
    "\n",
    "Now let's take a look at the values. Remember, the values in a dictionary are ways to describe the variable that belongs to a key.\n",
    "\n",
    "VimeoVideo(\"693794768\", h=\"8e5b53b0ca\", width=600)\n",
    "Task 5.1.8: Explore the values associated with the keys in poland_data. What do each of them represent? How is the information associated with the \"data\" key organized?\n",
    "\n",
    "# Continue Exploring `poland_data`\n",
    "#poland_data[\"metadata\"]\n",
    "#poland_data[\"schema\"].keys()\n",
    "poland_data[\"data\"][0]\n",
    "This dataset includes all the information we need to figure whether or not a Polish company went bankrupt in 2009. There's a bunch of features included in the dataset, each of which corresponds to some element of a company's balance sheet. You can explore the features by looking at the data dictionary. Most importantly, we also know whether or not the company went bankrupt. That's the last key-value pair.\n",
    "\n",
    "Now that we know what data we have for each company, let's take a look at how many companies there are.\n",
    "\n",
    "VimeoVideo(\"693794783\", h=\"8d333027cc\", width=600)\n",
    "Task 5.1.9: Calculate the number of companies included in the dataset.\n",
    "\n",
    "Calculate the length of a list in Python.\n",
    "List the keys of a dictionary in Python.\n",
    "# Calculate number of companies\n",
    "len(poland_data[\"data\"])\n",
    "9977\n",
    "And then let's see how many features were included for one of the companies.\n",
    "\n",
    "VimeoVideo(\"693794797\", h=\"3c1eff82dc\", width=600)\n",
    "Task 5.1.10: Calculate the number of features associated with \"company_1\".\n",
    "\n",
    "# Calculate number of features\n",
    "len(poland_data[\"data\"][0])\n",
    "66\n",
    "Since we're dealing with data stored in a JSON file, which is common for semi-structured data, we can't assume that all companies have the same features. So let's check!\n",
    "\n",
    "VimeoVideo(\"693794810\", h=\"80e195944b\", width=600)\n",
    "Task 5.1.11: Iterate through the companies in poland_data[\"data\"] and check that they all have the same number of features.\n",
    "\n",
    "What's an iterator?\n",
    "Access the items in a dictionary in Python.\n",
    "Write a for loop in Python.\n",
    "# Iterate through companies\n",
    "for item in poland_data[\"data\"]:\n",
    "    if len(item) != 66:\n",
    "        print(\"ALERT!!\")\n",
    "It looks like they do!\n",
    "\n",
    "Let's put all this together. First, open up the compressed dataset and load it directly into a dictionary.\n",
    "\n",
    "VimeoVideo(\"693794824\", h=\"dbfc9b43ee\", width=600)\n",
    "Task 5.1.12: Using a context manager, open the file poland-bankruptcy-data-2009.json.gz and load it as a dictionary with the variable name poland_data_gz.\n",
    "\n",
    "What's a context manager?\n",
    "Open a file in Python.\n",
    "Load a JSON file into a dictionary using Python.\n",
    "# Open compressed file and load contents\n",
    "with gzip.open(\"data/poland-bankruptcy-data-2009.json.gz\", \"r\") as read_file:\n",
    "    poland_data = json.load(read_file)\n",
    "print(type(poland_data_gz))\n",
    "<class 'dict'>\n",
    "Since we now have two versions of the dataset â€” one compressed and one uncompressed â€” we need to compare them to make sure they're the same.\n",
    "\n",
    "VimeoVideo(\"693794837\", h=\"925b5e4e5a\", width=600)\n",
    "Task 5.1.13: Explore poland_data_gz to confirm that is contains the same data as data, in the same format.\n",
    "\n",
    "# Explore `poland_data_gz`\n",
    "print(poland_data_gz.keys())\n",
    "print(len(poland_data_gz[\"data\"]))\n",
    "print(len(poland_data_gz[\"data\"][10]))\n",
    "dict_keys(['schema', 'data', 'metadata'])\n",
    "9977\n",
    "66\n",
    "Looks good! Now that we have an uncompressed dataset, we can turn it into a DataFrame using pandas.\n",
    "\n",
    "VimeoVideo(\"693794853\", h=\"b74ef86783\", width=600)\n",
    "Task 5.1.14: Create a DataFrame df that contains the all companies in the dataset, indexed by \"company_id\". Remember the principles of tidy data that you learned in Project 1, and make sure your DataFrame has shape (9977, 65).\n",
    "\n",
    "Create a DataFrame from a dictionary in pandas.\n",
    "df = pd.DataFrame().from_dict(poland_data_gz[\"data\"]).set_index(\"company_id\")\n",
    "print(df.shape)\n",
    "df.head()\n",
    "(9977, 65)\n",
    "feat_1\tfeat_2\tfeat_3\tfeat_4\tfeat_5\tfeat_6\tfeat_7\tfeat_8\tfeat_9\tfeat_10\t...\tfeat_56\tfeat_57\tfeat_58\tfeat_59\tfeat_60\tfeat_61\tfeat_62\tfeat_63\tfeat_64\tbankrupt\n",
    "company_id\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n",
    "1\t0.174190\t0.41299\t0.14371\t1.3480\t-28.9820\t0.60383\t0.219460\t1.12250\t1.1961\t0.46359\t...\t0.163960\t0.375740\t0.83604\t0.000007\t9.7145\t6.2813\t84.291\t4.3303\t4.0341\tFalse\n",
    "2\t0.146240\t0.46038\t0.28230\t1.6294\t2.5952\t0.00000\t0.171850\t1.17210\t1.6018\t0.53962\t...\t0.027516\t0.271000\t0.90108\t0.000000\t5.9882\t4.1103\t102.190\t3.5716\t5.9500\tFalse\n",
    "3\t0.000595\t0.22612\t0.48839\t3.1599\t84.8740\t0.19114\t0.004572\t2.98810\t1.0077\t0.67566\t...\t0.007639\t0.000881\t0.99236\t0.000000\t6.7742\t3.7922\t64.846\t5.6287\t4.4581\tFalse\n",
    "5\t0.188290\t0.41504\t0.34231\t1.9279\t-58.2740\t0.00000\t0.233580\t1.40940\t1.3393\t0.58496\t...\t0.176480\t0.321880\t0.82635\t0.073039\t2.5912\t7.0756\t100.540\t3.6303\t4.6375\tFalse\n",
    "6\t0.182060\t0.55615\t0.32191\t1.6045\t16.3140\t0.00000\t0.182060\t0.79808\t1.8126\t0.44385\t...\t0.555770\t0.410190\t0.46957\t0.029421\t8.4553\t3.3488\t107.240\t3.4036\t12.4540\tFalse\n",
    "5 rows Ã— 65 columns\n",
    "\n",
    "Import\n",
    "Now that we have everything set up the way we need it to be, let's combine all these steps into a single function that will decompress the file, load it into a DataFrame, and return it to us as something we can use.\n",
    "\n",
    "VimeoVideo(\"693794879\", h=\"f51a3a342f\", width=600)\n",
    "Task 5.1.15: Create a wrangle function that takes the name of a compressed file as input and returns a tidy DataFrame. After you confirm that your function is working as intended, submit it to the grader.\n",
    "\n",
    "def wrangle(filename):\n",
    "    #open compresesed file, load into dict\n",
    "    with gzip.open(filename, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "    #turn dict into dataframe\n",
    "    df = pd.DataFrame().from_dict(data[\"data\"]).set_index(\"company_id\")\n",
    "    return df\n",
    "\n",
    "\n",
    "df = wrangle(\"data/poland-bankruptcy-data-2009.json.gz\")\n",
    "print(df.shape)\n",
    "df.head()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
